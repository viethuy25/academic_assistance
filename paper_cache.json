{"2407.18213v1": {"title": "Exploring Scaling Trends in LLM Robustness", "authors": "Nikolhaus Howe, Micha\u0142 Zajac, Ian McKenzie, Oskar Hollinsworth, Tom Tseng, Pierre-Luc Bacon, Adam Gleave", "impact_score": 2.209, "keywords": ["Robustness", "LLMs", "Adversarial Training"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18213v1", "full_text": {"introduction": "", "methodology": "methods such as indirect prompt injections (Abdelnabi et al., 2023) to exploit LLM-driven applications without any awareness or participation by the user. A key question is whether future, more capable systems will naturally become more robust, or if this will instead require a dedicated safety effort. Although current attacks are concerning, the risks could grow still greater with future models capable of more dangerous actions, such as assisting with biological weapon development (Mouton et al., 2023), or with greater affordances to interact with the world (Sharkey et al., 2023), such as a virtual assistant for a CEO of a major company. Prior work has found that superhuman Go systems (Wang et al., 2023) are vulnerable to attack, demonstrating that impressive capabilities do not guarantee robustness. However, work has also found that scaling unlabeled pretraining data (Hendrycks et al., 2019; Carmon et al., 2022; Alayrac et al., 2019) and model size (Xie and Yuille, 2019; Huang et al., 2023) improves adversarial robustness in computer vision. To answer this question, we conduct an empirical investigation into scaling trends for the adversarial robustness of language models. These trends enable us to forecast the robustness of future models, and give us insight into how the offense-defense balance might shift over time. For example, does the cost of conducting an attack against more capable models grow faster or slower than the defender\u2019s cost of training those models? Concretely, we investigate the robustness of Pythia models ranging from 14M to 12B parameter (Biderman et al., 2023) against two attacks: the random tokens baseline and the state-of-the-art greedy coordinate gradient attack. We test these models in various simple classification tasks where our models achieve high accuracy on clean (non-adversarial) data. We first evaluate these pretrained models fine-tuned only on clean data. Larger models tend to be more resistant to attack, but the effect is weak and noisy (Figure 1). By contrast, a clearer scaling trend emerges for models adversarially trained against examples of attacks (Figure 2). Larger models are both more sample efficient, becoming more robust with fewer examples, and converge to be more robust given a sufficient number of examples. Moreover, adversarial training against one attack transfers protection to similar attacks, with the transfer being stronger for larger models (Figure 3(b)). Adversarial examples were first identified in image classifiers (Szegedy et al., 2014), but have since been found for systems performing image captioning (Xu et al., 2019; Zhang et al., 2020), speech recognition (Cisse et al., 2017; Alzantot et al., 2018; Sch\u00f6nherr et al., 2018), and reinforcement learning (Huang et al., 2017; Gleave et al., 2020; Ilahi et al., 2022). Moreover, a range of adversarial threat models (Gilmer et al., 2018) give rise to viable attacks. Most recently, many qualitatively different vulnerabilities have been found in language models, from human-understandable \u201cjailbreaks\u201d (Wei et al., 2023) to seemingly gibberish adversarial suffixes (Wallace et al., 2021; Zou et al., 2023). Simple methods such as perplexity filtering and paraphrasing defend against some of these attacks (Jain et al., 2023). However, these defenses can easily be bypassed by more sophisticated methods (Zhu et al., 2023). Adversarial training shows more promise as a defense (Ziegler et al., 2022), and is the focus of our analysis. The determinants of adversarial robustness have been well-studied in computer vision. One line of scholarship proposes a fundamental tradeoff between robustness and accuracy (Tsipras et al., 2019): exploitable models are simply relying on non-robust features (Ilyas et al., 2019), which improve training performance but hurt robustness. Other work has emphasized what does improve robustness. Scaling unlabeled pretraining data (Hendrycks et al., 2019; Carmon et al., 2022; Alayrac et al., 2019), model depth (Xie and Yuille, 2019) and model width (Huang et al., 2023) improves adversarial robustness in computer vision. However, other work shows that computer vision adversarial robustness scales too slowly to be a full solution (Debenedetti et al., 2023; Bartoldson et al., 2024). Language model scaling laws (Hestness et al., 2017; Rosenfeld et al., 2019; Kaplan et al., 2020; Hoffmann et al., 2022) have shown that increasing compute improves performance across many tasks and domains (Chen et al., 2021; Hernandez et al., 2021). However, scaling does not solve all problems (Lin et al., 2022; McKenzie et al., 2023). There has been only limited work on scaling laws for adversarial robustness in language models, with mixed", "model_architecture": "", "results": "results. Ganguli et al. (2022) show that LLMs become harder to attack with scale\u2014but Anil et al. (2024) find that some attacks become more successful with scale. We test models in the binary classification setting, as it is the simplest context in which to study LLM robustness. Crucially, binary classification allows us to measure robustness by the attack success rate, defined as the proportion of examples correctly classified by the model before attack that are incorrectly classified after attack.111We assume that the attack does not, in fact, change the ground truth label of the datapoint. This is guaranteed by construction for some of our simple procedurally generated tasks, and was manually validated on a random sample of datapoints in other tasks. We adapt pretrained models for classification by replacing the unembedding layer with a randomly initialized classification head, and then fine-tune the models on each task. Tasks We consider four tasks in our experiments, the latter two developed by us for this project: Spam (Metsis et al., 2006): Given the subject and body of an email, is it spam or not? IMDB (Maas et al., 2011): Given a movie review, is the sentiment positive or negative? PasswordMatch: Given two strings in the prompt, are they exactly equal? WordLength: Given two words in the prompt, is the first word shorter than the second? Spam and IMDB were chosen as standard natural language processing classification tasks. PasswordMatch was inspired by TensorTrust (Toyer et al., 2023), and WordLength by the RuLES dataset (Mu et al., 2023). Both PasswordMatch and WordLength were designed to be easily procedurally generated and have ground truth labels that can be checked algorithmically. For brevity, we report on Spam and IMDB in the main text, with plots for other tasks deferred to Appendices D and E. We provide example datapoints and details about the datasets in Appendix B. Models We test the Pythia model family (Biderman et al., 2023). These models range in size from 14M to 12B parameters (or 7.6M to 11.6B when used with a classification head). All models were trained to predict the next token on the same dataset following the same training protocol, allowing us to isolate model scale from other confounding factors. Attacks Our attacks append an adversarial suffix of N\ud835\udc41Nitalic_N tokens to the prompt. We use two different procedures to generate this adversarial suffix: a random token baseline (RandomToken) and the state-of-the-art greedy coordinate gradient attack (GCG; Zou et al., 2023). RandomToken was chosen due to its simplicity. GCG was chosen as it is currently one of the most effective attacks on language models. In the RandomToken baseline, the N\ud835\udc41Nitalic_N tokens are chosen uniformly at random from the model\u2019s vocabulary. We evaluate the model on the attacked text and then repeat the process with another sample of N\ud835\udc41Nitalic_N random tokens until the model is successfully attacked or an appointed budget for model calls is exhausted. In GCG (Zou et al., 2023), the N\ud835\udc41Nitalic_N tokens are initialized arbitrarily and then greedily optimized over multiple rounds. In each round, the gradient of the loss function with respect to the attack tokens is computed. This gradient is used to compute a set of promising single-token modifications, from which the best candidate is selected and used in the next round. To make this attack work in the classification setting, we minimize the cross-entropy loss between the predicted label and the target label. In our experiments, we always use N=10\ud835\udc4110N=10italic_N = 10 tokens. For more details about the attacks and hyperparameters used, see Appendix C. Figure 1 shows the robustness of fine-tuned models against the GCG attack. The attack is generally less successful on larger models, but model size alone does not explain all the variance in attack success rate. We observe similarly large random variation in attack success across model sizes on other tasks and with other attacks; for more details, see Appendix D.2. As described in Section 3, we use the Pythia models, which range from 7.6M to 11.6B parameters after replacing the unembedding matrix with a classification head.222In all figures, we report the actual parameter count of the classification model, and not the pretrained model it was derived from. We fine-tune all models for a single epoch with default hyperparameters from HuggingFace Transformers (Wolf et al., 2019), except for the learning rate which we set to 1\u2062e\u221251e51\\mathrm{e}{-5}1 roman_e - 5. All models reach >83%absentpercent83>83\\%> 83 % accuracy on all tasks, with larger models generally performing better (see Appendix D.1 for the final validation performance of all models on all tasks). We then evaluate the fine-tuned models against adversarial attacks on an unseen validation dataset. To understand the source of the variability in model robustness shown by our experiments, we varied 1) the pretraining checkpoint,333The Pythia models provide checkpoints from earlier stages of pretraining. We used various checkpoints from the final 10% of pretraining as a starting point for fine-tuning. and 2) the random seeds used to initialize the classification head before fine-tuning. Both factors led to significant variability in model robustness, with pretraining checkpoint contributing significantly more variability. The variability was comparable or greater than that of an order of magnitude of model scaling, indicating that out-of-the-box robustness on a given task is heavily influenced by the randomness of the pretraining procedure itself. This initial result suggests that we cannot rely on scale alone to solve the problem of robustness. However, in practice, we would apply a defense to a model prior to deploying it in a security-critical setting. In the following section, we consider whether scale enables defenses to more effectively improve model robustness. In this section, we explore how model size impacts robustness when performing adversarial training. Figure 2 evaluates the robustness of Pythia models to the GCG attack when adversarially trained against the same attack. We see a much cleaner trend than in the fine-tuning only case: larger models gain robustness more quickly and converge to be more robust than smaller models. These results suggest that model size is a strong predictor"}, "abstract": "Language model capabilities predictably improve from scaling a model's size\nand training data. Motivated by this, increasingly large language models have\nbeen trained, yielding an array of impressive capabilities. Yet these models\nare vulnerable to adversarial prompts, such as \"jailbreaks\" that hijack models\nto perform undesired behaviors, posing a significant risk of misuse. Prior work\nindicates that computer vision models become more robust with model and data\nscaling, raising the question: does language model robustness also improve with\nscale? We study this question empirically, finding that larger models respond\nsubstantially better to adversarial training, but there is little to no benefit\nfrom model scale in the absence of explicit defenses."}, "2407.18219v1": {"title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve", "authors": "Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar", "impact_score": 2.059, "keywords": ["self-improvement", "introspection", "LLMs"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18219v1", "full_text": {"introduction": "", "methodology": "approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions. A promising approach to utilizing and deploying foundation models, and in particular, large language models (LLMs), is to treat them as general-purpose decision-making machines, or \u201dagents\u201d. To be successful, an LLM agent must not just provide plausible completions for input text, but must exhibit interactive, goal-directed behavior to accomplish a given task. Put in abstract terms, this requires mastering two qualities: (a) producing responses that explicitly seek information about the task, followed by (b) making decisions and improving them by \u201dthinking\u201d and verifying them at inference time. For instance, to succeed in using a new coding library, an effective LLM agent should first synthesize programs, then try the most promising subset against a compiler, use the resulting feedback to improve the program, and repeat the process for multiple turns. Having the ability to successfully improve a response in sequential attempts is equivalent to a form of \u201dself-improvement\u201d, at test time. To enable test-time self-improvement, recent approaches attempt to repurpose the knowledge already stored in pre-trained models via few-shot prompting [15, 31, 52, 64, 7]. Although prompt tuning in conjunction with feedback is effective in eliciting improved responses from capable models, it fails to produce models that can succeed in complex tasks by correcting their own mistakes, such as those that require logical reasoning [21, 55]. In many of these problems, models contain the \u201cknowledge\u201d needed to answer a challenging prompt, but are not able to elicit that knowledge even when asked to sequentially correct their mistakes. Fine-tuning the LLM on domain-specific question-answering data [39, 6, 29] can help, but it still does not teach the agent a test-time improvement strategy (see Section 6). A strategy for improving responses over sequential attempts at test time is crucial for tackling challenging prompts, where directly attempting the problem in one shot may largely be futile. Can we train models to be capable of improving their own responses? If done correctly and on a diverse set of problems and scenarios, this could introduce in an LLM, a general procedure for \u201chow\u201d it can tackle a hard prompt by improving itself as opposed to supervising it with \u201cwhat\u201d to respond with, which may not generalize as the test prompt becomes out of distribution. Although one straightforward approach to inducing this capability into a model would be to generate data that showcase improvements over multiple sequential turns (potentially from highly capable models), we find that simply imitating these data is not sufficient to enable this capability (Section 6.4). Quite well, this is due to two reasons: First, multi-turn data from a different model would not show improvements in the kinds of errors the learner would make, thereby being irrelevant to the learner [24]. Second, often sequential multi-turn data collected from proprietary models is also not of high quality since these models are typically not good at proposing meaningful improvements to their own errors [21] even though they can still provide useful responses to the problem at hand. Therefore, we need a different strategy to endow models with a self-improvement capability. Our key insight is to supervise improvements to the learner\u2019s own responses in an iterative fashion, taking inspiration from methods in online imitation learning [36] and reinforcement learning (RL) [45]. This supervision can be in the form of oracle responses to the prompt sampled i.i.d. from more capable models, or be generated from the learner itself. Our contribution is an algorithm RISE: Recursive Introspection (Figure 1) that utilizes these insights to improve the self-improvement capability of an LLM over the course of multiple attempts at a given prompt. In each iteration, our approach bootstraps on-policy rollouts from the learner with better responses at the next turn obtained by running best-of-N (using a success indicator on the task) on multiple revision candidates obtained by sampling from the learner itself or using responses from a more capable model, whichever is more convenient. In this way, we are able to construct rollouts that demonstrate the learner how it can improve its responses under its own distribution. Then, we fine-tune the learner on these data using a reward-weighted regression (RWR [35, 34]) objective, that is able to learn from both high- and low-quality parts of such rollouts. By iteratively repeating this procedure, we are able to instill a general self-improvement capability into an LLM. Our", "model_architecture": "", "results": "results show that LLMs trained via RISE can produce correct responses on more prompts, improving over turns for more challenging prompts. Even though strong base and intruction-tuned LLMs [23, 58] often fail to improve their responses over multiple sequential attempts (even when explicitly told about their mistakes previously), RISE successfully endows similarly-sized LLMs with self-improvement capabilities, resulting in monotonically increasing task performance after each turn. Specifically, on the GSM8K [11] dataset, RISE demonstrates significant improvement over various models. RISE improves the performance of LLaMa3-8B by 8.2% and Mistral-7B by 6.6%, entirely using their own data. RISE attains a 17.7% improvement for LLaMa2-7B over the course of 5-turn introspection (outperforming parallel sampling from the first turn), and a 23.9% improvement for Mistral-7B. In contrast, GPT-3.5 itself only improves by 4.6% over five turns. We see similar trends on the MATH dataset [18], where RISE improves LLaMa2-7B by 4.6% and Mistral-7B by 11.1% over five turns. We also study why and how RISE is able to induce self-improvement abilities and show that this ability generalizes to out-of-distribution prompts as well. These results consistently demonstrate RISE\u2019s effectiveness in enhancing mathematical reasoning capabilities for different models. Several prior works build techniques to improve reasoning and thinking capabilities of foundation models for downstream applications. Typically these works focus on building prompting techniques for effective multi-turn interaction with external tools [54, 7, 5, 32, 56, 49, 14], sequentially refining predictions by reflecting on actions [7, 15, 63], asking the model to verbalize its thoughts [52, 33, 65], asking the model to critique and revise itself [31, 40] or by using other models to critique a primary model\u2019s responses [12, 54, 2, 20]. Although a subset of this work does improve its own responses, this self-correction ability often requires access to detailed error traces (e.g., execution traces from code compilers [31, 7]) in order to succeed. In fact, [21] and Table 1 both indicate that self-improvement guided by the LLM itself (i.e., \u201cintrinsic self-correction\u201d) is often infeasible for off-the-shelf LLMs even when they contain the knowledge required to tackle the prompt given, but fine-tuning with RISE induces this capability as we show in this paper. Beyond prompting, previous work also attempts to fine-tune LLM to obtain self-improvement capabilities [39, 6, 62]. These works attempt to improve reasoning performance by training on self-generated responses [58, 60, 30, 46, 57]. To achieve this, these works use a combination of learned verifiers [50, 28, 47], search [26, 33, 13, 38], contrastive prompting on negative data [9, 48], and iterated supervised or reinforcement learning (RL) [8, 59, 37]. Although our approach also trains on model-generated data, we aim to introduce a complementary capability to improve performance over sequential turns of interaction, rather than to improve single-turn performance alone. Other work fine-tunes LLMs for multi-turn interaction directly via RL [41, 66]: while this is indeed related, single-turn problems posed in multi-turn scenarios require addressing distinct challenges than generic multi-turn RL: (i) sample-efficiency is not a concern since the entire environment is fully characterized by the training dataset of prompts and oracle answers and dynamics are deterministic, and (ii) we need to generalize to novel test prompts. Multi-turn RL focuses on sample efficiency, which is not as critical in our setting, though of course learning to generalize from a limited number of initial states would be appealing. Our main focus is to show that it is possible to train models for self-improvement via appropriately designing multi-turn fine-tuning objectives. This is orthogonal from the choice of training approach (RL or not). The most related to our work are GLoRE [17] and Self-Correct [53], which train separate models to identify errors and refine incorrect answers of other LLMs. Unlike these works, our approach trains a single model to produce answers and improve them over more than two turns, which is the maximal number of turns studied in these works. We show that doing so successfully requires careful design choices: an iterative on-policy data generation strategy along with a training objective that can learn from both successful and unsuccessful rollouts. From an algorithmic point of view, RISE is similar to online imitation learning [36, 44], in that it queries expert supervision on states attained by on-policy rollouts. On-policy distillation for LLMs [1, 4] utilizes this idea, but queries an expert to provide completions on partial responses instead of sequential attempts, that we do in this work. The goal of our work is to improve LLM performance over sequential attempts / turns at a given problem. Concretely, given a dataset \ud835\udc9f={(\ud835\udc31i,\ud835\udc32i\u2217)}i=1N\ud835\udc9fsuperscriptsubscriptsubscript\ud835\udc31\ud835\udc56subscriptsuperscript\ud835\udc32\ud835\udc56\ud835\udc561\ud835\udc41\\mathcal{D}=\\{(\\mathbf{x}_{i},\\mathbf{y}^{*}_{i})\\}_{i=1}^{N}caligraphic_D = { ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_y start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT of problems \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathbf{x}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and oracle responses \ud835\udc32i\u2217subscriptsuperscript\ud835\udc32\ud835\udc56\\mathbf{y}^{*}_{i}bold_y start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, our goal is to obtain an LLM \u03c0\u03b8(\u22c5|[\ud835\udc31,\ud835\udc32^1:t,p1:t])\\pi_{\\theta}(\\cdot|[\\mathbf{x},\\hat{\\mathbf{y}}_{1:t},p_{1:t}])italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( \u22c5 | [ bold_x , over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT ] ) that, given the problem \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, previous model attempts \ud835\udc32^1:tsubscript^\ud835\udc32:1\ud835\udc61\\hat{\\mathbf{y}}_{1:t}over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT at the problem, and auxiliary instructions p1:tsubscript\ud835\udc5d:1\ud835\udc61p_{1:t}italic_p start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT (e.g., instruction to find a mistake and improve the response; or additional compiler feedback from the environment) solves a given problem as correctly as possible. To this end, we encode this goal into the following learning objective that we wish to optimize: Unlike standard supervised fine-tuning that trains the model \u03c0\ud835\udf0b\\piitalic_\u03c0 to produce a single response \ud835\udc32^^\ud835\udc32\\hat{\\mathbf{y}}over^ start_ARG bold_y end_ARG given \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, Equation 3.1 trains \u03c0\ud835\udf0b\\piitalic_\u03c0 to also appropriately react to a given history of responses from its own previous attempts \ud835\udc32^1:i\u22121subscript^\ud835\udc32:1\ud835\udc561\\hat{\\mathbf{y}}_{1:i-1}over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT. Equation 3.1 most closely resembles an RL objective, and we will indeed develop our approach by converting a single-turn problem into a multi-turn MDP. Finally, note that prompting-based methods such as Self-Refine [31] can still be viewed as training \u03c0\ud835\udf0b\\piitalic_\u03c0 to optimize \u03c0\u2062(\ud835\udc32\u2217|\ud835\udc31)\ud835\udf0bconditionalsuperscript\ud835\udc32\ud835\udc31\\pi(\\mathbf{y}^{*}|\\mathbf{x})italic_\u03c0 ( bold_y start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT |"}, "abstract": "A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions."}, "2407.18242v1": {"title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?", "authors": "Zhengbo Wang, Jian Liang", "impact_score": 2.359, "keywords": ["LoRA", "Fine-tuning", "NLP"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18242v1", "full_text": {"introduction": "", "methodology": "method for parameter-efficient fine-tuning foundation models by re-parameterizing the original matrix into the product of two low-rank matrices. Despite its efficiency, LoRA often yields inferior performance compared to full fine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap. Firstly, we delve into the optimization processes in LoRA and full fine-tuning. We reveal that while LoRA employs low-rank approximation, it neglects to approximate the optimization process of full fine-tuning. To address this, we introduce a novel concept called the \"equivalent gradient.\" This virtual gradient makes the optimization process on the re-parameterized matrix equivalent to LoRA, which can be used to quantify the differences between LoRA and full fine-tuning. The equivalent gradient is derived from the gradients of matrices A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B. To narrow the performance gap, our approach minimizes the differences between the equivalent gradient and the gradient obtained from full fine-tuning during the optimization process. By solving this objective, we derive optimal closed-form solutions for updating matrices A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B. Our method constrains the optimization process, shrinking the performance gap between LoRA and full fine-tuning. Extensive experiments on natural language processing tasks validate the effectiveness of our method. Foundational models [Radford et al., 2021, Brown et al., 2020, Achiam et al., 2023, Kirillov et al., 2023, Rombach et al., 2022] have become the cornerstone of modern deep learning. By undergoing pre-training on massive datasets, these models typically exhibit excellent generalization and versatility. Remarkably, some foundation models even demonstrate emergent properties [Hoffmann et al., 2022, Kaplan et al., 2020]. As a result, foundation models have been widely applied to various downstream applications. Despite these advantages, the huge number of parameters in foundational models hinders their broader application. The substantial parameter count", "model_architecture": "", "results": "results in high fine-tuning costs for these tasks. To address this issue, recent research has focused on parameter-efficient fine-tuning (PEFT) methods [Hu et al., 2022, Houlsby et al., 2019, Lester et al., 2021, Zhou et al., 2022]. PEFT methods reduce the fine-tuning cost by keeping the foundation models frozen and only fine-tuning small, additional lightweight adapters. With the majority of parameters frozen, PEFT enables faster fine-tuning and requires fewer computational resources. Low-rank adaptation [Hu et al., 2022], also known as LoRA, is one of the most famous PEFT methods, which has been widely adopted across various domains. Inspired by previous works [Aghajanyan et al., 2021, Li et al., 2018], LoRA hypothesizes that the changes in weights during model adaptation exhibit a low-rank structure. To capture this, LoRA re-parameterizes these changes by expressing them as the product of two low-rank matrices: W=W0+\u0394\u2062W\u2248W0+s\u2062B\u2062A\ud835\udc4asubscript\ud835\udc4a0\u0394\ud835\udc4asubscript\ud835\udc4a0\ud835\udc60\ud835\udc35\ud835\udc34W=W_{0}+\\Delta W\\approx W_{0}+sBAitalic_W = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + roman_\u0394 italic_W \u2248 italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_s italic_B italic_A, where s\ud835\udc60sitalic_s is a scaling factor, and A\u2208\u211dr\u00d7n\ud835\udc34superscript\u211d\ud835\udc5f\ud835\udc5bA\\in\\mathbb{R}^{r\\times n}italic_A \u2208 blackboard_R start_POSTSUPERSCRIPT italic_r \u00d7 italic_n end_POSTSUPERSCRIPT and B\u2208\u211dm\u00d7r\ud835\udc35superscript\u211d\ud835\udc5a\ud835\udc5fB\\in\\mathbb{R}^{m\\times r}italic_B \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m \u00d7 italic_r end_POSTSUPERSCRIPT are low-rank matrices with rank r\u226amin\u2061(m,n)much-less-than\ud835\udc5f\ud835\udc5a\ud835\udc5br\\ll\\min(m,n)italic_r \u226a roman_min ( italic_m , italic_n ). LoRA reduces the number of trainable parameters from m\u00d7n\ud835\udc5a\ud835\udc5bm\\times nitalic_m \u00d7 italic_n to r\u00d7(m+n)\ud835\udc5f\ud835\udc5a\ud835\udc5br\\times(m+n)italic_r \u00d7 ( italic_m + italic_n ), thereby decreasing the cost of fine-tuning. However, despite its efficiency, LoRA\u2019s fine-tuning performance often falls short compared to full fine-tuning [Hu et al., 2022, Liu et al., 2024, Ding et al., 2023]. In this paper, we propose a novel PEFT method, LoRA-Pro, aimed at bridging the gap between LoRA and full fine-tuning. While LoRA employs low-rank approximation by re-parametrizing weight changes as the product of two low-rank matrices, it falls short in approximating the optimization process of full fine-tuning. To measure their discrepancy in the optimization process, we propose a novel concept, \u201cEquivalent Gradient\", for LoRA optimization. Equivalent gradient characterizes the gradient of the original matrix after low-rank approximation (despite it not being directly trainable), is composed of gradients from matrices A and B. Thus, during LoRA fine-tuning, our goal is not only to approximate the matrix with low-rank matrices but also to minimize the difference between the equivalent gradient and the gradient from full fine-tuning during the gradient descent process. This is achieved by selecting appropriate gradients for matrices A and B, ensuring a more accurate and effective fine-tuning process. To achieve this, we formulate it as an optimization problem. We then derive theoretical solutions for the problem, presenting optimal gradients for updating matrices A and B. These solutions ensure that the equivalent gradient closely match the optimization dynamics of full fine-tuning. By doing so, we enhance the effectiveness LoRA, bridging the gap between LoRA and full fine-tuning. Our main contributions are summarized as follows: We identify that LoRA approximates low-rank matrices but neglects to approximate the optimization process of full parameter fine-tuning. This shortcoming is one of the reasons for the performance gap between LoRA and full fine-tuning. We introduce the concept of Equivalent Gradient, which allows us to quantify the discrepancy in the optimization process between LoRA and full fine-tuning. By minimizing this discrepancy, we derive the optimal closed-form updated solutions for LoRA. Extensive experiments on natural language processing tasks validate the effectiveness of our method. Parameter-Efficient Fine-Tuning. Given the huge size of foundation models, recent research has focused on developing parameter-efficient fine-tuning methods [Hu et al., 2022, Liu et al., 2024, Ding et al., 2023, Houlsby et al., 2019, Liu et al., 2023, Lester et al., 2021]. These methods aim to reduce the cost of fine-tuning by adjusting only a small portion of the model\u2019s parameters. Generally, these methods fall into two main categories. The first category is adapter tuning [Houlsby et al., 2019, Sung et al., 2022, He et al., 2021, Zhang et al., 2024, Bapna and Firat, 2019, Hu et al., 2022], which involves inserting small neural network modules, called adapters, into specific layers of the model. During fine-tuning, we keep the model frozen and only fine-tune the lightweight adapter modules, significantly reducing the memory footprint for fine-tuning. The second category is prompt tuning [Lester et al., 2021, Zhou et al., 2022, Li and Liang, 2021, Liu et al., 2022]. Prompt tuning adapts the models to specific tasks by adding specially designed prompts or learnable tokens to the input data, rather than directly modifying the internal parameters of foundation models. In this paper, we focus on LoRA [Hu et al., 2022], a prominent method within the realm of adapter tuning. Low Rank Adaptation. Low-rank adaptation, initially referred to as LoRA [Hu et al., 2022], has evolved into a broad category encompassing parameter-efficient fine-tuning methods based on low-rank approximations [Hu et al., 2022, Liu et al., 2024, Hayou et al., 2024, Kalajdzievski, 2023, Zhang et al., 2023, Kopiczko et al., 2024, Hyeon-Woo et al., 2022, Zhang and Pilanci, 2024, Wang et al., 2024, Zhao et al., 2024]. LoRA [Hu et al., 2022] assumes that the changes in the weights of pre-trained models exhibit a low-rank structure. Consequently, it re-parameterizes these changes as the product of low-rank matrices, thereby reducing the cost associated with fine-tuning. Several variants of LoRA have been proposed to address different aspects of this approach. For example, DoRA [Liu et al., 2024] improves LoRA [Hu et al., 2022] by incorporating a learnable magnitude vector to re-scale the normalized product of low-rank matrices. Another variant, rsLoRA Kalajdzievski [2023], introduces a new scaling factor to stabilize training in high-rank scenarios. LoRA+[Hayou et al., 2024] improves upon LoRA by applying different learning rates to the two low-rank matrices. Additionally, Galore [Zhao et al., 2024] employs SVD to project the gradients of full parameter training into a low-rank space, thereby reducing the memory footprint during pre-training and fine-tuning. In this section, we begin by revisiting LoRA [Hu et al., 2022] in Section 3.1. Following this, we conduct a comparison between LoRA and full fine-tuning from an optimization perspective in Section 3.2. Finally, in Section 3.3, we point"}, "abstract": "Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the \"equivalent gradient.\" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method."}, "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?": "## LoRA-Pro: Are Low-Rank Adapters Properly Optimized?\n\n**Authors:** Zhengbo Wang, Jian Liang\n**Impact Score:** 2.359\n**Keywords:** LoRA, Fine-tuning, NLP\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18242v1\n\n**Summary:**\nHere is a breakdown of the research paper 'LoRA-Pro: Are Low-Rank Adapters Properly Optimized?' in bullet points:\n\n**1. Methodology**\n- The paper addresses the performance gap between LoRA, a low-rank adaptation technique for efficient fine-tuning, and full fine-tuning. \n- To bridge this gap, the research proposes LoRA-Pro, which introduces a novel concept called \"equivalent gradient.\" This virtual gradient represents the gradient of the original matrix after low-rank approximation, derived from gradients of matrices A and B. By minimizing the difference between this equivalent gradient and the gradient obtained from full fine-tuning, LoRA-Pro aims to align the optimization process with full fine-tuning. \n- The paper derives optimal closed-form solutions for updating matrices A and B, which are validated through experiments on natural language processing tasks.\n\n**2. Content Rating:** 65-79\n\n**3. Rating Explanation:**\n- The paper presents a novel approach with the potential to improve LoRA's performance, but further validation and comparison with other techniques are necessary.\n\n**4. Key Findings:**\n- The research demonstrates that LoRA-Pro can narrow the performance gap between LoRA and full fine-tuning, but specific quantitative measures of the performance difference are not provided.\n- The paper explores the concept of \"equivalent gradient\" and derives optimal solutions for updating matrices A and B. The effectiveness of these solutions is validated through experiments on NLP tasks.\n\n**5. Potential Improvements:**\n- The paper could benefit from a more comprehensive analysis of the performance gap between LoRA and full fine-tuning, including quantitative measures and comparisons with other methods.\n- Further research could investigate the applicability of LoRA-Pro to other domains and model architectures beyond NLP. \n", "Recursive Introspection: Teaching Language Model Agents How to Self-Improve": "## Recursive Introspection: Teaching Language Model Agents How to Self-Improve\n\n**Authors:** Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar\n**Impact Score:** 2.059\n**Keywords:** self-improvement, introspection, LLMs\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18219v1\n\n**Summary:**\nHere is a breakdown of the research paper 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve' in bullet points:\n\n### Methodology\n\n- The addressed issue is the lack of self-improvement capabilities in LLMs, where even with access to the required knowledge, they often fail to correct their mistakes in sequential attempts.\n- The research design utilizes an iterative fine-tuning procedure called RISE, which involves training LLMs on multi-turn data generated from their own responses. This is achieved by posing fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP) and utilizing strategies inspired by online imitation learning and reinforcement learning.\n- The methodology relies on a specific task of mathematical reasoning and utilizes a set of established datasets, including GSM8K and MATH. The reliability of the methodology is supported by its ability to demonstrate significant improvements in the self-improvement capabilities of LLMs across different models. However, the applicability and generalizability of the approach to diverse tasks and domains beyond mathematical reasoning require further investigation.\n\n### Content Rating: 65\n\n### Rating Explanation\n\n- The paper provides a clear and concise methodology that addresses a significant issue within the field of LLM research. The chosen methodology is well-supported by the findings, exhibiting strong evidence of its efficacy. However, the scope of the research is limited to a specific task, and further investigation is needed to assess its broader applicability.\n\n### Key Finding\n\n- The research demonstrates that LLMs trained via RISE exhibit significant improvements in their ability to self-correct over multiple turns, outperforming baseline models and achieving notable gains in accuracy on mathematical reasoning tasks. This finding highlights the potential of RISE to enhance the problem-solving capabilities of LLMs.\n- The paper convincingly demonstrates that RISE effectively instills self-improvement capabilities into LLMs, enabling them to learn from their mistakes and iteratively refine their responses. However, the study focuses primarily on the effectiveness of RISE on specific mathematical reasoning tasks, and further research is needed to assess its efficacy across a wider range of tasks and domains.\n\n### Potential Improvement\n\n- Future research could investigate the generalizability of RISE to other domains and tasks beyond mathematical reasoning, ensuring its effectiveness and applicability in various contexts. \n- Additionally, exploring the use of diverse training datasets and incorporating feedback from different sources would enhance the robustness and versatility of the self-improvement approach.\n", "Exploring Scaling Trends in LLM Robustness": "## Exploring Scaling Trends in LLM Robustness\n\n**Authors:** Nikolhaus Howe, Micha\u0142 Zajac, Ian McKenzie, Oskar Hollinsworth, Tom Tseng, Pierre-Luc Bacon, Adam Gleave\n**Impact Score:** 2.209\n**Keywords:** Robustness, LLMs, Adversarial Training\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18213v1\n\n**Summary:**\nHere is a breakdown of the research paper 'Exploring Scaling Trends in LLM Robustness' in bullet points:\n\n**1. Methodology**\n- This paper explores the impact of model size and adversarial training on the robustness of language models (LLMs) against adversarial attacks, specifically focusing on \"jailbreaks\" that manipulate models for undesirable behavior.\n- The study utilizes Pythia models ranging in size from 14M to 12B parameters, subjecting them to two attack types: a random token baseline and a state-of-the-art greedy coordinate gradient attack (GCG). The research design includes evaluating both fine-tuned models and adversarially trained models to analyze the scaling trends in robustness. \n-  The methodology involves a controlled experiment with multiple models and attacks, offering a structured approach to evaluating the impact of scale. However, it relies on a limited set of tasks and a single family of models, which may limit the generalizability of the findings. \n\n**2. Content Rating:** 65\n\n**3. Rating Explanation:**\n- The research provides valuable insights into the scaling trends of LLM robustness, contributing to the ongoing discussion of securing LLMs.\n- The limited scope of tasks and model families could be addressed with further investigation for broader impact.\n\n**4. Key Findings:**\n- The study indicates that larger models demonstrate improved resistance to adversarial attacks when subjected to adversarial training, exhibiting a clearer scaling trend compared to models fine-tuned solely on clean data.\n- The findings suggest that model size plays a crucial role in enabling effective adversarial training, leading to more robust models with greater sample efficiency and better convergence. \n\n**5. Potential Improvement:**\n- Further investigation into a broader range of tasks, attack types, and model architectures is needed to assess the generalizability and robustness of the observed scaling trends. \n- The study could benefit from exploring additional defense mechanisms beyond adversarial training to gain a more comprehensive understanding of LLM robustness. \n", "2407.18148v1": {"title": "StraightLine: An End-to-End Resource-Aware Scheduler for Machine Learning Application Requests", "authors": "Cheng-Wei Ching, Boyuan Guan, Hailu Xu, Liting Hu", "impact_score": 1.759, "keywords": ["scheduling", "resource allocation", "machine learning"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18148v1", "full_text": {"introduction": "", "methodology": "method to trade-off the required computational resources to the provisioned memory. Figure 8 shows the comparison of the response time of running \\acML applications on different computing platforms. Clearly, the Flask API outperforms the rest. This is because the Flask API can offer local computation for \\acML applications to directly call the inference function. Both Docker and AWS serverless solutions cause longer response times because of the Docker container activation overhead. Traditional ML systems focus on one particular stage or phase of the life cycle of ML applications. These systems often aim at optimizing model training or accelerating model inference, and they frequently assume homogeneous infrastructure for model training or model inference, which may not always reflect real-world scenarios. In this paper, we present StraightLine, an end-to-end resource-aware scheduler that schedules the optimal resources (e.g., container, virtual machine, or serverless process) for different ML application requests in a hybrid infrastructure. StraightLine leverages Docker to containerize the stages in model development to offer plug-and-go provisioning. StraightLine proposes an empirical dynamic placing algorithm that intelligently places requests based on their unique characteristics (e.g., request frequencies and request data sizes). We have conducted preliminary experiments to demonstrate StraightLine\u2019s effectiveness in reducing response time and failure rate. In the future, we plan to explore the following directions: (1) evaluate StraightLine across diverse scenarios and workloads and compare its performance with alternative approaches; (2) enhance StraightLine\u2019s empirical dynamic placing algorithm to consider additional parameters of the models and characteristics of the applications (e.g., SLOs); and (3) further refine StraightLine to dynamically allocate resources based on real-time demand fluctuations and workload patterns, ensuring optimal resource utilization and performance.", "model_architecture": "", "results": "results of Flask API implemented on the local web server and the in-house data center. From Figure 4(a) we see that the local web server and in-house data center achieve a similar failed rate when total sessions reach 1300. From Figures 4(b) and 4(c) we can see that the session length in the local web server surges when total sessions reach 1200, whereas the session length in the in-house data center increases when total sessions reach 1400. This is because the web server implements a single thread to run the \\acML application. Serverless computing. Figure 5 shows the results of session length, and response time of AWS Lambda stack implemented with 2GB and 3GB Docker. The performance has improved significantly on the AWS Lambda stack. We can see that the median response time stays around 300-500 ms even when the request frequency increases up to 6000 per 180 seconds and the failed rate reaches up to 60% when the request frequency rises to 6000 per 180 seconds. The results on AWS Lambda outperform the traditional RESTful API implementations. Hybrid Infrastructure. Figure 8 and Figure 8 show the comparison of the failed rate of running \\acML applications on different computing platforms. It is obvious that as the request frequency increases, serverless computing shows an acceptable failed rate. In addition, as the provisioned memory increased from 2GB to 3GB for serverless computing, the failed rate decreased. Therefore, serverless computing tends to be a good solution for high request frequencies. This is because serverless computing can offer unlimited resources for running \\acML applications as well as an efficient orchestration"}, "abstract": "The life cycle of machine learning (ML) applications consists of two stages:\nmodel development and model deployment. However, traditional ML systems (e.g.,\ntraining-specific or inference-specific systems) focus on one particular stage\nor phase of the life cycle of ML applications. These systems often aim at\noptimizing model training or accelerating model inference, and they frequently\nassume homogeneous infrastructure, which may not always reflect real-world\nscenarios that include cloud data centers, local servers, containers, and\nserverless platforms. We present StraightLine, an end-to-end resource-aware\nscheduler that schedules the optimal resources (e.g., container, virtual\nmachine, or serverless) for different ML application requests in a hybrid\ninfrastructure. The key innovation is an empirical dynamic placing algorithm\nthat intelligently places requests based on their unique characteristics (e.g.,\nrequest frequency, input data size, and data distribution). In contrast to\nexisting ML systems, StraightLine offers end-to-end resource-aware placement,\nthereby it can significantly reduce response time and failure rate for model\ndeployment when facing different computing resources in the hybrid\ninfrastructure."}, "2407.18181v1": {"title": "Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning", "authors": "Sindhura Kommu, Yizhi Wang, Yue Wang, Xuan Wang", "impact_score": 2.359, "keywords": ["scRNA-seq", "GRN", "graph learning"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18181v1", "full_text": {"introduction": "", "methodology": "approach scTransNet that combines the rich contextual representations learned by pre-trained single-cell language models with the structured knowledge encoded in GRNs using graph neural networks (GNNs). By integrating these two modalities, our approach effectively reasons over both the gene expression level constraints provided by the scRNA-seq data and the structured biological knowledge inherent in GRNs. We evaluate scTransNet on human cell benchmark datasets from the BEELINE study with cell type-specific ground truth networks. The", "model_architecture": "", "results": "results demonstrate superior performance over current state-of-the-art baselines, offering a deeper understanding of cellular regulatory mechanisms. Single-cell RNA sequencing (scRNA-seq) has transformed the exploration of gene expression patterns at the individual cell level (Jovic et al., 2022), offering an unprecedented opportunity to unravel the intricate regulatory mechanisms governing cellular identity and function (Pratapa et al., 2020). One such promising application is the inference of gene regulatory networks (GRNs) which represent the complex interplay between transcription factors (TFs) and their downstream target genes (Akers & Murali, 2021; Cramer, 2019). A precise understanding of GRNs is crucial for understanding cellular processes, molecular functions, and ultimately, developing effective therapeutic interventions (Biswas et al., 2021). However, inferring GRNs from scRNA-seq data is challenging due to cell heterogeneity (Wagner et al., 2016), cell cycle effects (Buettner et al., 2015), and high sparsity caused by dropout events (Kharchenko et al., 2014), which can impact accuracy and robustness. Additionally, the availability of labeled scRNA-seq data corresponding to a GRN is limited, making it challenging to train models from scratch. Traditional unsupervised or self-supervised models, while not reliant on label information, often struggle to effectively handle the noise, dropouts, high sparsity, and high dimensionality characteristics of scRNA-seq data (Moerman et al., 2019; Matsumoto et al., 2017; Zeng et al., 2023). Supervised methods are also proposed for GRN reconstruction (Zhao et al., 2022; Shu et al., 2022; KC et al., 2019; Chen & Liu, 2022a) but struggle to handle batch effects and fail to leverage latent gene-gene interaction information effectively limiting their generalization capabilities. Recent advancements in large language models (LLMs) and the pre-training followed by fine-tuning paradigm (Devlin et al., 2019; OpenAI, 2023) have significantly contributed to the development of transformer-based architectures tailored for scRNA-seq data analysis (Yang et al., 2022; Cui et al., 2024; Chen et al., 2023; Theodoris et al., 2023). These models effectively leverage vast amounts of unlabeled scRNA-seq data to learn contextual representations and capture intricate latent interactions between genes. To address the limitations of the current methods, we effectively leverage one of these large-scale pre-trained transformer models, namely scBERT (Yang et al., 2022), which has been pre-trained on large-scale unlabelled scRNA-seq data to learn domain-irrelevant gene expression patterns and interactions from the whole genome expression. By fine-tuning scBERT on user specific scRNA-seq datasets, we can mitigate batch effects and capture latent gene-gene interactions for downstream tasks. We propose an innovative knowledge-aware supervised GRN inference framework, scTransNet (see Figure 1), which integrates pre-trained single-cell language models with structured knowledge of GRNs. Our approach combines gene representations learned from scBERT with graph representations derived from the corresponding GRNs, creating a unified context-aware and knowledge-aware representation (Feng et al., 2020). This joint learning approach enables us to surpass the accuracy of current state-of-the-art methods in supervised GRN inference. By harnessing the power of pre-trained transformer models and incorporating biological knowledge from diverse data sources, such as gene expression data and gene regulatory networks, our approach paves the way for more precise and robust GRN inference. Ultimately, this methodology offers deeper insights into cellular regulatory mechanisms, advancing our understanding of gene regulation. Several methods have been developed to infer GRNs from scRNA-seq data, broadly categorized into unsupervised and supervised methods. Unsupervised methods primarily include information theory-based, model-based, and machine learning-based approaches. Information theory-based methods, such as mutual information (MI) (Margolin et al., 2006), Pearson correlation coefficient (PCC) (Salleh et al., 2015; Raza & Jaiswal, 2013), and partial information decomposition and context (PIDC) (Chan et al., 2017), conduct correlation analyses under the assumption that the strength of the correlation between genes is is positively correlated with the likelihood of regulation between them. Model-based approaches, such as SCODE (Matsumoto et al., 2017), involve fitting gene expression profiles to models that describe gene relationships, which are then used to reconstruct GRNs (Shu et al., 2021; Tsai et al., 2020). Machine learning-based unsupervised methods, like GENIE3 (Huynh-Thu et al., 2010) and GRNBoost2 (Moerman et al., 2019), utilize tree-based algorithms to infer GRNs. These methods are integrated into tools like SCENIC (Aibar et al., 2017; Van de Sande et al., 2020), employing tree rules to learn regulatory relationships by iteratively excluding one gene at a time to determine its associations with other genes. Despite not requiring labeled data, these unsupervised methods often struggle with the noise, dropouts, high sparsity, and high dimensionality typical of scRNA-seq data. Additionally, the computational expense and scalability issues of these tree-based methods, due to the necessity of segmenting input data and iteratively establishing multiple models, present further challenges for large datasets. Supervised methods, including DGRNS (Zhao et al., 2022), convolutional neural network for co-expression (CNNC) (Yuan & Bar-Joseph, 2019), and DeepDRIM (Chen et al., 2021), have been developed to address the increasing scale and inherent complexity of scRNA-seq data. Compared with unsupervised learning, supervised models are capable of detecting much more subtle differences between positive and negative pairs (Yuan & Bar-Joseph, 2019). DGRNS (Zhao et al., 2022) combines recurrent neural networks (RNNs) for extracting temporal features and convolutional neural networks (CNNs) for extracting spatial features to infer GRNs. CNNC (Yuan & Bar-Joseph, 2019) converts the identification of gene regulation into an image classification task by transforming the expression values of gene pairs into histograms and using a CNN for classification. However, the performance of CNNC (Yuan & Bar-Joseph, 2019) is hindered by the issue of transitive interactions. To address this, DeepDRIM (Chen et al., 2021) considers the information from neighboring genes and converts TF\u2013gene pairs and neighboring genes into histograms as additional inputs, thereby reducing the occurrence of transitive interactions to some extent. Despite their success there exist certain limitations to the employment of CNN model-based approaches for GRN reconstruction. First of all, the generation of image data not only gives rise to unanticipated noise but also conceals certain original data features. Additionally, this process is time-consuming, and since it changes the format of scRNA-seq data, the predictions made by these CNN-based computational approaches cannot be wholly explained. In addition to CNN-based methods, there are also other"}, "abstract": "Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing\n(scRNA-seq) data is a complex challenge that requires capturing the intricate\nrelationships between genes and their regulatory interactions. In this study,\nwe tackle this challenge by leveraging the single-cell BERT-based pre-trained\ntransformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to\naugment structured biological knowledge from existing GRNs. We introduce a\nnovel joint graph learning approach that combines the rich contextual\nrepresentations learned by pre-trained single-cell language models with the\nstructured knowledge encoded in GRNs using graph neural networks (GNNs). By\nintegrating these two modalities, our approach effectively reasons over boththe\ngene expression level constraints provided by the scRNA-seq data and the\nstructured biological knowledge inherent in GRNs. We evaluate our method on\nhuman cell benchmark datasets from the BEELINE study with cell type-specific\nground truth networks. The results demonstrate superior performance over\ncurrent state-of-the-art baselines, offering a deeper understanding of cellular\nregulatory mechanisms."}, "2407.18202v1": {"title": "Differentiable Quantum Architecture Search in Asynchronous Quantum Reinforcement Learning", "authors": "Samuel Yen-Chi Chen", "impact_score": 2.359, "keywords": ["Quantum Architecture Search", "Asynchronous RL", "QRL"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18202v1", "full_text": {"introduction": "", "methodology": "methods facilitating parallel training. Through numerical simulations, we demonstrate that our proposed DiffQAS-QRL approach achieves performance comparable to manually-crafted circuit architectures across considered environments, showcasing stability across diverse scenarios. This methodology offers a pathway for designing QRL models without extensive quantum knowledge, ensuring robust performance and fostering broader application of QRL. Quantum computing (QC) theoretically possesses the potential to fundamentally transform computational tasks, presenting clear advantages over classical computers [1]. The advancement in QC hardware, coupled with classical ML techniques, enables the progression of quantum machine learning (QML). This is realized through the hybrid quantum-classical computing paradigm [2, 3], wherein both classical and quantum computers are harnessed. Specifically, computational tasks suited for QC capabilities are executed on quantum computers, while tasks such as gradient calculations, well-handled by classical computers, remain within their domain. The variational quantum circuit (VQC) serves as the fundamental component in existing QML methodologies. A plethora of QML models, based on VQC, have been developed to address various machine learning (ML) tasks including classification [4, 5, 6, 7, 8, 9], time-series prediction [10, 11, 12], generative modeling [13, 14], natural language processing [15, 16, 17, 18, 19], and reinforcement learning [20, 21, 22, 23, 24, 25, 26]. Despite the achievements of various QML models, a significant challenge hindering widespread adoption is the necessity for extensive expertise in designing effective quantum circuit architectures. For instance, crafting the structure of encoding and parameterized circuits within the VQC demands specialized design considerations, including appropriate entanglement to showcase quantum advantages. There exists a pressing demand for an automated procedure capable of streamlining the search for high-performing quantum circuit architectures. In this manuscript, we address this challenge through the implementation of differentiable quantum architecture search (DiffQAS). Our objective is to integrate DiffQAS into quantum reinforcement learning (QRL), as reinforcement learning (RL) stands out in the realm of ML for its handling of sequential decision-making problems and potential to exhibit high-level problem-solving capabilities. Specifically, we examine a selection of VQC block candidates and assign trainable structural weights to these blocks. Through gradient descent optimization, we concurrently learn these structural weights alongside the conventional quantum circuit parameters (rotation angles). Furthermore, departing from prior works in QAS for QRL, we train the agents using asynchronous training rather than single-process policy updates, thereby leveraging the computational resources of multi-core CPUs or potentially multiple quantum processing units (QPUs) in the future. Numerical simulations are employed to illustrate the efficacy of the proposed DiffQAS-QRL framework in identifying VQC architectures capable of achieving high scores in diverse testing environments. Specifically, we demonstrate the stability of the proposed method across various environments, contrasting with the inconsistent performance of manually-designed architectures across different scenarios. This observation underscores the necessity for a task-agnostic automated procedure in designing QRL circuits and underscores the effectiveness of the proposed DiffQAS-QRL framework. This manuscript is structured as follows: Section II presents a concise overview of the current advancements in QAS and QRL. In Section III, the fundamental concepts of quantum RL and VQC are elucidated, forming the foundational components of extant QML and QRL models, which represent the focus of inquiry for the proposed framework. The formulation of the differentiable QAS problem is delineated in Section IV, while the intricacies of the proposed DiffQAS-QRL framework are expounded upon in Section V, along with the methodologies for numerical simulation and the corresponding outcomes discussed in Section VI. Ultimately, the", "model_architecture": "", "results": "findings are summarized in Section VII. Quantum reinforcement learning (QRL) has been a subject of exploration since the groundbreaking work by Dong et al. in 2008 [27]. Initially, its practicality was hindered by the requirement to construct environments entirely in a quantum fashion, thus limiting its real-world applicability. However, subsequent advancements in QRL, leveraging variational quantum circuits (VQCs), have broadened its horizons to encompass classical environments with both discrete [23] and continuous observation spaces [24, 25]. The evolution of QRL has witnessed performance improvements through the adoption of policy-based learning methodologies, including Proximal Policy Optimization (PPO) [28], Soft Actor-Critic (SAC) [29], REINFORCE [26], Advantage Actor-Critic (A2C) [30], and Asynchronous Advantage Actor-Critic (A3C) [31]. Moreover, in addressing the challenges presented by partially observable environments, researchers have explored the utilization of quantum recurrent neural networks such as quantum LSTM as RL policies [20, 21]. Recent advancements also encompass hybrid models, wherein a classical neural network is trained to dynamically adjust the parameters of the quantum circuit. This enables the model to address intricate sequential decision-making tasks without relying on quantum recurrence [32]. Nevertheless, the accomplishments mentioned in QRL necessitate profound expertise in designing high-performing quantum circuit architectures to leverage potential quantum advantages. Consequently, there is an imminent requirement to develop automated procedures for designing quantum circuit architectures to cater to the demands of various application domains. Machine learning techniques have been employed to address various challenges in quantum computing, such as quantum architecture search (QAS). The objectives of QAS may include generating desired quantum states [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43], discovering efficient circuits for solving chemical ground states [44, 45, 46, 43, 47], addressing optimization tasks [48, 49, 45, 50, 43, 51, 52], optimizing quantum circuits for specific hardware architectures [53], compiling circuits [54, 55, 56], or conducting machine learning tasks [57, 49, 50, 58, 59, 60, 61, 62, 63]. Various methodologies are employed to discover the optimal circuit for specific tasks. For instance, reinforcement learning-based approaches are explored in works such as [33, 34, 53, 44, 48, 35, 36, 38, 40, 41, 56], while different variants of evolutionary algorithms are utilized in works like [57, 58, 59, 39] to search for circuits. Additionally, differentiable QAS methods have been developed to leverage gradient-based techniques effectively [50, 51, 52, 61]. Various approaches to encode quantum circuit architecture have been proposed, with some utilizing graph-based methods as seen in works such as [49, 46], while others, like [53], consider convolutional neural network-based methods. As for circuit performance metrics, they may involve direct evaluation of circuit performance on specific tasks [44, 57, 45], or assessing the proximity of the generated circuit to the actual circuit [33, 34, 49]. To alleviate the computational resources needed for direct evaluation, predictor-based methods have been proposed, employing neural networks to predict quantum model performance without direct circuit evaluation [47, 62]. The proposed framework in this paper extends the asynchronous training of QRL described in the work [31, 21, 32] to include the capability of differentiable QAS to search for the best performing circuit. Our work also differentiates from the previous work [61] as our work considers the asynchronous training which can leverage parallel computing resource or in the future, multiple-QPU environments. Our work is also different from the previous work such as [57] since our work leverages the differentiable method, unlike the evolutionary methods requiring a large amount of performance evaluation. Quantum machine learning largely depends on the trainable quantum circuits: variational quantum circuit (VQC), also known as parameterized quantum circuits (PQC). The VQC, as shown in Figure 2, usually has three basic components: encoding circuit U\u2062(x\u2192)\ud835\udc48\u2192\ud835\udc65U(\\vec{x})italic_U ( over\u2192 start_ARG italic_x end_ARG ), variational circuit V\u2062(\u03b8\u2192)\ud835\udc49\u2192\ud835\udf03V(\\vec{\\theta})italic_V ( over\u2192 start_ARG italic_\u03b8 end_ARG ) and the final measurement part. The purpose of encoding circuit U\u2062(x\u2192)\ud835\udc48\u2192\ud835\udc65U(\\vec{x})italic_U ( over\u2192 start_ARG italic_x end_ARG ) is to transform the input vector x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG into a quantum state U\u2062(x\u2192)\u2062|0\u27e9\u2297n\ud835\udc48\u2192\ud835\udc65superscriptket0tensor-productabsent\ud835\udc5bU(\\vec{x})\\ket{0}^{\\otimes n}italic_U ( over\u2192 start_ARG italic_x end_ARG ) | start_ARG 0 end_ARG \u27e9 start_POSTSUPERSCRIPT \u2297 italic_n end_POSTSUPERSCRIPT, where |0\u27e9\u2297nsuperscriptket0tensor-productabsent\ud835\udc5b\\ket{0}^{\\otimes n}| start_ARG 0 end_ARG \u27e9 start_POSTSUPERSCRIPT \u2297 italic_n end_POSTSUPERSCRIPT is the ground state of the quantum system and n\ud835\udc5bnitalic_n represents the number of the qubit. The encoded state then go through the variational circuit and becomes V\u2062(\u03b8\u2192)\u2062U\u2062(x\u2192)\u2062|0\u27e9\u2297n\ud835\udc49\u2192\ud835\udf03\ud835\udc48\u2192\ud835\udc65superscriptket0tensor-productabsent\ud835\udc5bV(\\vec{\\theta})U(\\vec{x})\\ket{0}^{\\otimes n}italic_V ( over\u2192 start_ARG italic_\u03b8 end_ARG ) italic_U ( over\u2192 start_ARG italic_x end_ARG ) | start_ARG 0 end_ARG \u27e9 start_POSTSUPERSCRIPT \u2297 italic_n end_POSTSUPERSCRIPT. To retrieve the information from the VQC, measurements can be carried out with pre-defined observables B^ksubscript^\ud835\udc35\ud835\udc58\\hat{B}_{k}over^ start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. The VQC operation can be seen as as quantum function f\u2062(x\u2192;\u03b8\u2192)\u2192=(\u27e8B^1\u27e9,\u22ef,\u27e8B^n\u27e9)\u2192\ud835\udc53\u2192\ud835\udc65\u2192\ud835\udf03delimited-\u27e8\u27e9subscript^\ud835\udc351\u22efdelimited-\u27e8\u27e9subscript^\ud835\udc35\ud835\udc5b\\overrightarrow{f(\\vec{x};\\vec{\\theta})}=\\left(\\left\\langle\\hat{B}_{1}\\right% \\rangle,\\cdots,\\left\\langle\\hat{B}_{n}\\right\\rangle\\right)over\u2192 start_ARG italic_f ( over\u2192 start_ARG italic_x end_ARG ; over\u2192 start_ARG italic_\u03b8 end_ARG ) end_ARG = ( \u27e8 over^ start_ARG italic_B end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u27e9 , \u22ef , \u27e8 over^ start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u27e9 ), where \u27e8B^k\u27e9=\u27e80|U\u2020\u2062(x\u2192)\u2062V\u2020\u2062(\u03b8\u2192)\u2062B^k\u2062V\u2062(\u03b8\u2192)\u2062U\u2062(x\u2192)|0\u27e9delimited-\u27e8\u27e9subscript^\ud835\udc35\ud835\udc58quantum-operator-product0superscript\ud835\udc48\u2020\u2192\ud835\udc65superscript\ud835\udc49\u2020\u2192\ud835\udf03subscript^\ud835\udc35\ud835\udc58\ud835\udc49\u2192\ud835\udf03\ud835\udc48\u2192\ud835\udc650\\left\\langle\\hat{B}_{k}\\right\\rangle=\\left\\langle 0\\left|U^{\\dagger}(\\vec{x})V% ^{\\dagger}(\\vec{\\theta})\\hat{B}_{k}V(\\vec{\\theta})U(\\vec{x})\\right|0\\right\\rangle\u27e8 over^ start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u27e9 = \u27e8 0 | italic_U start_POSTSUPERSCRIPT \u2020 end_POSTSUPERSCRIPT ( over\u2192 start_ARG italic_x end_ARG ) italic_V start_POSTSUPERSCRIPT \u2020 end_POSTSUPERSCRIPT ( over\u2192 start_ARG italic_\u03b8 end_ARG ) over^ start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_V ( over\u2192 start_ARG italic_\u03b8 end_ARG ) italic_U ( over\u2192 start_ARG italic_x end_ARG ) | 0 \u27e9. Expectation values \u27e8B^k\u27e9delimited-\u27e8\u27e9subscript^\ud835\udc35\ud835\udc58\\left\\langle\\hat{B}_{k}\\right\\rangle\u27e8 over^ start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u27e9 can be obtained by performing multiple samplings (shots) on actual quantum hardware or through direct computation when utilizing simulation software. Reinforcement learning (RL) constitutes a ML paradigm wherein an agent endeavors to achieve a predefined objective or goal through interactions with an environment \u2130\u2130\\mathcal{E}caligraphic_E within discrete time intervals [64]. At each time step t\ud835\udc61titalic_t, the agent perceives a state stsubscript\ud835\udc60\ud835\udc61s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and subsequently selects an action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT from the action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A based on its prevailing policy \u03c0\ud835\udf0b\\piitalic_\u03c0. The policy signifies a mapping from a specific state stsubscript\ud835\udc60\ud835\udc61s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to the probabilities associated with selecting an action from \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A. Upon executing action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, the agent receives a scalar reward rtsubscript\ud835\udc5f\ud835\udc61r_{t}italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the updated subsequent state"}, "abstract": "The emergence of quantum reinforcement learning (QRL) is propelled by\nadvancements in quantum computing (QC) and machine learning (ML), particularly\nthrough quantum neural networks (QNN) built on variational quantum circuits\n(VQC). These advancements have proven successful in addressing sequential\ndecision-making tasks. However, constructing effective QRL models demands\nsignificant expertise due to challenges in designing quantum circuit\narchitectures, including data encoding and parameterized circuits, which\nprofoundly influence model performance. In this paper, we propose addressing\nthis challenge with differentiable quantum architecture search (DiffQAS),\nenabling trainable circuit parameters and structure weights using\ngradient-based optimization. Furthermore, we enhance training efficiency\nthrough asynchronous reinforcement learning (RL) methods facilitating parallel\ntraining. Through numerical simulations, we demonstrate that our proposed\nDiffQAS-QRL approach achieves performance comparable to manually-crafted\ncircuit architectures across considered environments, showcasing stability\nacross diverse scenarios. This methodology offers a pathway for designing QRL\nmodels without extensive quantum knowledge, ensuring robust performance and\nfostering broader application of QRL."}, "2407.18151v1": {"title": "ArtA: Automating Design Space Exploration of Spin Qubit Architectures", "authors": "Nikiforos Paraskevopoulos, David Hamel, Aritra Sarkar, Carmen G. Almudever, Sebastian Feld", "impact_score": 2.359, "keywords": ["quantum", "architecture", "optimization"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18151v1", "full_text": {"introduction": "", "methodology": "method configurations, significantly reducing exploration times by up to 99.1% compared to a traditional brute force approach while maintaining the same result quality. After a comprehensive evaluation of best-matching optimization configurations per quantum circuit, ArtA suggests universal architectural features that perform optimally across all examined circuits, emphasizing the importance of maximizing quantum gate parallelization at the expense of more crosstalk interference. As the field of quantum computing rapidly advances, different qubit technologies exhibit unique hardware and performance characteristics. It still remains to be seen which one (e.g., superconducting, trapped ions, quantum dots, photonics, defect-based on nitrogen-vacancy diamond centers) will succeed in scaling up quantum computing systems with high-quality qubits [55, 11]. Among them, spin qubits in quantum dots emerge as a compelling avenue for achieving scalability for practical quantum computation [11]. To this day, spin qubits are at an early stage in their development, with Intel\u2019s Tunnel Falls chip currently boasting the highest count of twelve spin qubits [32]. Despite this, their inherent scalability advantages suggest that a robust two-dimensional design [63, 36, 7] could be scaled up relatively easily once fabrication techniques and quality improve up to a certain level. Designing a chip, however, involves multiple architectural design choices whose impact in the future can only be fully assessed through experimental studies post-scaling the technology. Exploring all these design possibilities simultaneously can be expensive, time-intensive, and impractical. Therefore, simulating how different design choices can affect performance in a range of quantum applications is crucial to assist the development. This process facilitates highlighting architectural insights that can guide the technology forward and will allow quantum researchers to make more informed decisions, streamline efforts, and hasten the development of these devices. Recognizing the intricate challenges of designing quantum processor architectures, this study initiates the first Design Space Exploration (DSE) for spin-qubit architectures, both from a current technological perspective and a future one. Therefore, in this work, we have identified a wide range of representative architectural features and abstracted them into usable input variables, resulting in 29,3122931229,31229 , 312 different architectures. To facilitate this exploration, we have enhanced the compilation capabilities of SpinQ compilation framework [47] to handle all these input variables and updated the definition of the Estimated Success Probability (ESP) metric to include not only operational errors but also crosstalk and decoherence errors. These transformations and many more establish SpinQ as the first compilation and DSE framework for spin qubit architectures. DSE processes [50, 39], which rely on a brute force approach to traverse and subsequently analyze the design space, will be impractical time-wise, especially for such large spaces. Our approach, however, employs ArtA (Artificial Architect), a built-in tool containing multiple optimization methods designed to automate the DSE process of SpinQ, thus taking less time than the brute force approach with the same quality of", "model_architecture": "", "results": "results. The abilities of ArtA are (a) to suggest which of the seventeen optimization configurations can find the architecture with the desired ESP the fastest compared to the brute-force approach and (b) which architectural design characteristics are key for building high-performance spin-qubit devices per quantum benchmark circuit. Our results demonstrate ArtA\u2019s ability to compare all optimization techniques for each quantum circuit and obtain a solution up to 99.1% faster, on average, than brute-forcing. Then, equipped with these insights, we move on to conduct a DSE analysis of 29,3122931229,31229 , 312 spin qubit architectures and provide valuable insights into best design practices. Firstly, we highlight the critical need to maximize the parallelization of quantum gates rather than minimizing crosstalk between qubits. Secondly, we find that the communication method with shuttle operations is better in large-scale circuits than SWAPs, but for single-qubit gates, pulse-based methods rotations are preferred over shuttle-based. Finally, our universal architecture for all used quantum circuits suggests that the likelihood of success increases when prioritizing the parallelization of single-qubit gates instead of two-qubit gates. The main contributions of this paper: The design space definition comprising of abstracted characteristics of current and, potentially, future spin qubit architectures. The new SpinQ, the first compiler and DSE framework for spin qubit architectures. In this version, we have updated the ESP formula to include operational, crosstalk, and decoherence-induced errors. ArtA, the first tool consisting of seventeen optimization method configurations automating the DSE process of spin qubit architectures. Evaluation of best-matching optimization configurations per quantum circuit. Evaluation of best-matching architectural characteristics per quantum circuit and universally (i.e., for all used circuits) in terms of ESP. The remainder of this paper is structured as follows: In Section II, we discuss the scalability potential of quantum-dot spin qubits as well as the importance of addressing engineering challenges while taking into account the resulting performance of the final architecture. Then, in Section III, we motivate the need for an automated Design Space Exploration analysis for spin qubit devices and formulate four research questions for this work. After that, in Section IV, we define the design space consisting of various input architectural variables and establish the new DSE functionality of SpinQ. Another upgrade in SpinQ is the enhanced Estimated Success Probability metric, described in Section V, which is used as the Figure of Merit for our exploration. In Section VI, we introduce ArtA (Artificial Architect) and two new metrics to evaluate its performance. In the results Section VII, we first evaluate ArtA\u2019s performance across all used quantum circuits and determine the best optimization method configuration. Then, we conduct a detailed analysis of the best architectural designs for each quantum circuit and form valuable insights for a universal architecture. We conclude our work and provide ideas for future directions in Section IX. Spin-qubit technologies are distinguished by their unique physical features, which position them as a highly scalable solution for quantum computing. The advantages of spin-qubits include a significantly smaller size \u2014 up to a thousand times less than other qubit technologies \u2014 combined with decades of semiconductor manufacturing expertise, long coherence times coupled with short gate durations, and high temperatures [71, 10, 30, 11, 74, 38, 63, 66, 72, 68]. At the core is the quantum dot, which contains a trapped electron(s) or hole(s) to form a physical qubit [26]. Spin qubits are manipulated electromagnetically using multiple precision-engineered gate electrodes that facilitate either single- or two-qubit operations through exact timing of pulse sequences across various quantum dot configurations. Recent studies have expanded these systems into one-dimensional and two-dimensional arrays [30, 6], exploring different structures and material combinations. Despite the mentioned advantages, spin qubit quantum processors are not as advanced as other qubit technologies in terms of qubit counts and availability. Major technological hurdles are related to the so-called interconnect bottleneck [63], and various fabrication challenges towards scaling up [9, 5, 18, 17]. On the upside, there have been significant efforts [36, 8, 63, 31, 21, 45, 48, 65] to tackle these challenges. However, solving them can not guarantee successful quantum algorithm executions. This is because the quality and quantity of qubits are not the only factors determining a high-performing quantum processor. The architectural constraints qubits need to comply with are equally important in how they are operated. For instance, the benefits of qubits with excellent operational fidelity can easily be outweighed by low qubit connectivity and limited natively supported quantum gates. Similarly, high-quality qubits with low crosstalk interference are not enough when they can not be addressed in parallel. These, and many more, are interlinked architectural trade-offs that affect the actual performance of the quantum processors. During the Noisy Intermediate-Scale Quantum (NISQ) era [51], predicting which architectural features and trade-offs will facilitate successful quantum circuit executions while maintaining reasonable hardware requirements remains challenging. To date, there has not been a systematic study exploring spin-qubit architectures through this prism of providing concrete guidelines for future development. Therefore, in this study, we exploit such an opportunity and alleviate the need for time-consuming, expensive, and technology-dependent experimental studies. The need to apply design space exploration (DSE) techniques for designing and optimizing full-stack quantum computing systems, or components thereof, has been emphasized in previous work [2]. These techniques have been successfully applied across different levels of the quantum computing stack. For instance, DSE has been used to explore ion-trap quantum processors [41] and evaluate compilation techniques for superconducting qubits [54]. Such techniques can abstractly convert architectural characteristics into design variables and, in this way, quantize the design space, expressing current and future design possibilities that otherwise would be impossible or too time-consuming to physically realize. Starting from there [50, 39], the steps for a proper DSE: i) Describe the problem in terms of input variables or parameters (design choices), ii) select the performance metrics, iii) choose a global cost function as a Figure of Merit that combines different performance metrics, and iv) find models (behavioral, analytical, experimental data) that connect input parameters with metrics (or directly to the Figure of Merit). By"}, "abstract": "In the fast-paced field of quantum computing, identifying the architectural\ncharacteristics that will enable quantum processors to achieve high performance\nacross a diverse range of quantum algorithms continues to pose a significant\nchallenge. Given the extensive and costly nature of experimentally testing\ndifferent designs, this paper introduces the first Design Space Exploration\n(DSE) for quantum-dot spin-qubit architectures. Utilizing the upgraded SpinQ\ncompilation framework, this study explores a substantial design space\ncomprising 29,312 spin-qubit-based architectures and applies an innovative\noptimization tool, ArtA (Artificial Architect), to speed up the design space\ntraversal. ArtA can leverage seventeen optimization method configurations,\nsignificantly reducing exploration times by up to 99.1% compared to a\ntraditional brute force approach while maintaining the same result quality.\nAfter a comprehensive evaluation of best-matching optimization configurations\nper quantum circuit, ArtA suggests universal architectural features that\nperform optimally across all examined circuits, emphasizing the importance of\nmaximizing quantum gate parallelization at the expense of more crosstalk\ninterference."}, "2407.18145v1": {"title": "Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for Open-World Perception", "authors": "Julia Hindel, Daniele Cattaneo, Abhinav Valada", "impact_score": 2.359, "keywords": ["Semantic Segmentation", "Continual Learning", "Hyperbolic Space"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18145v1", "full_text": {"introduction": "background. Extensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0 benchmarks demonstrate that it achieves state-of-the-art performance. We make the code and trained models publicly available at http://topics.cs.uni-freiburg.de. Automated vehicles rely on scene semantics predicted from online sensor data [mohan2022perceiving] as well as HD maps [greve2023collaborative] for safe navigation. The dominant paradigm for scene understanding exploits semantic [gosala2023skyeye] or panoptic segmentation models [kappeler2023few] trained on a dataset with a fixed number of predetermined semantic categories. However, such vehicles operate in an open-world scenario where training data with new object classes appear over time. While one line of research focuses on detecting unknown objects [mohan2024panoptic], Class-Incremental Learning (CIL) aims to update the model with new classes at periodic timesteps [zhou2023deep]. On one hand, training a new model from scratch every time new classes appear is not only computationally inefficient but also requires past and present data to be available. On the other hand, simply updating a trained model with new data will result in catastrophic forgetting of old knowledge as the model will be biased towards new classes [cermelli2023comformer]. Consequently, CIL methods aim to balance observing characteristics of new classes while preserving patterns of formerly learned classes as the model is evaluated on all seen classes [zhou2023deep]. Class-Incremental Semantic Segmentation (CISS) incorporates the background shift as an additional challenge. This phenomenon occurs as pixels that belong to old classes are labeled as background in new data samples [cermelli2020mib]. Consequently, CISS methods need to address label inconsistencies, catastrophic forgetting, and generalization on new classes at the same time. State-of-the-art CISS methods restrain the forgetting of old knowledge with data replay, network expansion, or regularization. The latter focuses on constraining features of the new model to imitate those of the prior model with direct feature distillation [Douillard2020PLOPLW] or frozen old class weights [zhang22_microseg, cha2021_ssul]. We find that these restrictions significantly hinder the plasticity of the model as old class features cannot evolve. Furthermore, most methods are tailored to the highly curated PascalVOC dataset which deviates significantly from densely annotated automated driving scenarios. While two to three object categories appear per image in PascalVOC, driving datasets typically contain over twenty different object categories, and fewer pixels are assigned to the background class. Additionally, all CISS methods assume that new classes originate from the prior background. This scenario is unrealistic for automated driving as a change in requirements for navigation could also entail bifurcations of previously observed classes for better decision-making, e.g. a model is initially trained to uniformly segment humans but later this ability needs to be extended to distinguish different vulnerable road users. In this work, we introduce taxonomy-aware continual semantic segmentation for automated driving scenarios. Our proposed Taxonomy-Oriented Poincar\u00e9-regularized Incremental Class Segmentation (TOPICS) approach enforces features conform to taxonomy-tree structures in hyperbolic space. As a result, the overall class distribution is rigid and new classes are appended at fitting positions in Fig. 1. This supervision also provides plasticity for old classes as the positions of ancestors are updated according to new classes. We show that this regularization is beneficial for open-world scenarios since it includes aspects of plasticity and rigidity. To further avoid catastrophic forgetting, we incorporate pseudo-labeling of the background and relation constraints of old class hyperplanes in TOPICS. We argue that semantic classes inherit relations that go beyond the defined class taxonomy such as similar appearances or contexts. Those relations can be observed as close mappings in latent space. We ensure consistency of the relations between prior class hyperplanes to increase the rigidity of our model. Lastly, we constrain features to have equidistant radii to maintain constant scarcity. We ensure that prior class features can only move in a circular direction around the hyperbolic center. Accordingly, new classes cannot result in a latent space shift of the complete taxonomy in their favor. We perform extensive evaluations of TOPICS on the standard Cityscapes [Cordts2016Cityscapes] and Mapillary Vistas 2.0 [neuhold2017mapillary] benchmarks where it sets the new state-of-the-art. Our main contributions can thus be summarized as follows: TOPICS, a taxonomic-aware modeling in hyperbolic space to balance plasticity and rigidity for CISS. Two novel regularization losses tailored for incremental learning in hyperbolic space. Extensive evaluations and ablation study under eight different universal CISS settings for autonomous driving. Publicly available code and pretrained models at http://topics.cs.uni-freiburg.de. In this section, we summarize existing works in class-incremental semantic segmentation, hyperbolic neural networks, and hierarchical learning. Class-Incremental Semantic Segmentation: CISS methods rely on data replay, expansion, or distillation to avoid catastrophic forgetting. Prior data is recreated with GANs [maracani2021recall] or a small subset of prior data is stored in memory-buffers [cha2021_ssul]. To further reduce memory constraints, prior work stores selected feature representations instead of raw data [chen2023STAR]. Expansion-based methods dedicate separate network components for particular semantic knowledge. For example, one branch of parallel convolutions adapts to the new data and is merged into the frozen branch after every incremental step [zhang2022representation]. Follow-up work extends this idea by fusing only endpoints of a trainable and frozen model in combination with distillation [xiao2023endpoints]. Distillation approaches maintain prior model weights to restrain the current model for equivalent responses to the input data [zhou2023deep]. The pioneering approach MiB [cermelli2020mib] relates prior background logits to the combination of novel-class and background logits in the new model. This method is enhanced with gradient-based attribution weight initialization which identifies relevant classifier weights for novel classes from prior weights of the background class [goswami2023attribution]. On the other hand, PLOP [Douillard2020PLOPLW] labels the background with prior model predictions and distills pooled intermediate feature representations. Subsequent work focuses on learning an enhanced weighting term for distillation [wang24] or adapting this principle to transformer architectures [cermelli2023comformer, qiu2023sats]. The method SATS [qiu2023sats] also highlights the benefit of relation distillation between self-attention vectors in a SegFormer model. This weaker constraint allows the model to avoid forgetting while not constraining its plasticity. Prior work also trains segmentation models with sigmoid activation and binary cross-entropy loss as the instability of softmax activations hinders incremental learning [cha2021_ssul]. DKD [baek2022_dkd] further combines this", "methodology": "methods impose strict rigidity on old classes, reducing their effectiveness in learning new incremental classes. In this work, we propose Taxonomy-Oriented Poincar\u00e9-regularized Incremental-Class Segmentation (TOPICS) that learns feature embeddings in hyperbolic space following explicit taxonomy-tree structures. This supervision provides plasticity for old classes, updating ancestors based on new classes while integrating new classes at fitting positions. Additionally, we maintain implicit class relational constraints on the geometric basis of the Poincar\u00e9 ball. This ensures that the latent space can continuously adapt to new constraints while maintaining a robust structure to combat catastrophic forgetting. We also establish eight realistic incremental learning protocols for autonomous driving scenarios, where novel classes can originate from known classes or the", "model_architecture": "", "results": "results in the hyperplanes of old classes rotating around the center and prevents a shift of the complete taxonomic tree in favor of the new classes. New space is allocated for new classes while the respective scarcity of old classes is not affected. In this section, we present quantitative and qualitative results of TOPICS on nine CISS settings in addition to a comprehensive ablation study to underline the importance of our contributions. Further, we detail the applied CISS settings and the training protocol that we employ. We evaluate TOPICS on the Cityscapes [Cordts2016Cityscapes] and Mapillary Vistas 2.0 [neuhold2017mapillary] datasets. For both datasets, we define CISS protocols where incremental classes either primarily originate from the background or known classes. We only consider the more realistic case of overlapped CISS where image pixels can belong to old, current, or future classes at any timestep. The Cityscapes dataset consists of 19 semantic classes in addition to a void class. For CISS from the background, we adapt the 14-1 (6 tasks) and 10-1 (10 tasks) setting as proposed in [goswami2023attribution]. The first 10 or 14 classes are learned during base training while one class is added per incremental step. The task count includes base training as the first task. For CISS from known classes, we learn 7 base classes that correspond to the official sub-categories defined for Cityscapes and increment the model in a 7-4 (4 tasks) or 7-18 (2 tasks) manner. For Mapillary Vistas 2.0, we leverage 111 valid semantic classes and collapse all void pixels into one background class. Consequently, we define the settings of 51-30 (3 tasks), and 71-10 (5 tasks) for CISS from the background. On the other hand, we evaluate taxonomic incremental capabilities with 39-84 (2 tasks) and 39-21 (5 tasks) on this dataset. For both datasets, we use the official validation split for testing and split the training data into training vs. validation with an 80:20 ratio. We note that the validation and test data remain constant for all incremental steps. For CISS from known classes, we divide the dataset into base and incremental dataset splits according to the number of learned classes within each step. In line with prior work [Douillard2020PLOPLW, cermelli2020mib, zhang22_microseg], we use the DeepLabV3 model with the ResNet-101 backbone which is pre-trained on ImageNet for all the experiments. We employ the Geoopt library [geoopt2020kochurov] to project the Euclidean features to a Poincar\u00e9 ball with c=2.0\ud835\udc502.0c=2.0italic_c = 2.0 (different curvatures are explored in Sec. LABEL:sec:curv). Further, we follow the M\u00f6bius approximation defined in [atigh2022hyperbolic] for more efficient computations. We train TOPICS for 60 epochs per task with batch size 24242424 for Cityscapes and 16161616 for Mapillary Vistas 2.0 using the Riemannian SGD optimizer with momentum of 0.90.90.90.9 and weight decay of 0.00010.00010.00010.0001. We use a poly learning rate scheduler with initial learning rates of 0.050.050.050.05 for base training and 0.010.010.010.01 in all incremental steps. We additionally ablate lower learning rates in Sec. IV-D2. For the hierarchical loss function, we set the hyper-parameters to \u03b1=5\ud835\udefc5\\alpha=5italic_\u03b1 = 5 and \u03b2=1\ud835\udefd1\\beta=1italic_\u03b2 = 1 and ablate different hierarchical functions in Sec. LABEL:sec:hh. For Mapillary Vistas 2.0, we rescale the longest size to 2177 pixels before taking a non-empty crop of (1024,1024) and horizontal flipping. On the other hand, we train on random non-empty crops of (512,1024) with horizontal flipping for Cityscapes. Non-empty cropping biases image crops to include labeled masks (i.e. new classes) which could be neglected when applying random cropping. We compare TOPICS with five state-of-the-art CISS methods: PLOP [Douillard2020PLOPLW], MiB [cermelli2020mib], MiB+AWT [goswami2023attribution], DKD [baek2022_dkd] and MicroSeg [zhang22_microseg]. For each method, we use the respective author\u2019s published code and use the same augmentations outlined in Sec. IV-B. For Cityscapes, we train the method PLOP on 512\u00d7512512512512\\times 512512 \u00d7 512 crops as the method is restricted to squared input images. We evaluate the models using the mean intersection-over-union (mIoU) metric. Specifically, we evaluate the mIoU over all the base classes (\ud835\udc9e1subscript\ud835\udc9e1\\mathcal{C}_{1}caligraphic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) and novel classes (\ud835\udc9e2:Tsubscript\ud835\udc9e:2\ud835\udc47\\mathcal{C}_{2:T}caligraphic_C start_POSTSUBSCRIPT 2 : italic_T end_POSTSUBSCRIPT) separately as an indication of rigidity and plasticity. We present the results on Cityscapes in Tab. I. On this dataset, TOPICS outperforms all baselines by at least 9.889.889.889.88pp on the CISS from the background. While the difference in base IoU measures 4.24.24.24.2pp, our method significantly exceeds the benchmarks by at least 16.916.916.916.9pp in terms of novel IoU. This finding emphasizes that a balance between plasticity and rigidity is crucial to achieving superior results for class incremental learning. Further, we note the largest performance difference on incremental scenarios from known classes where TOPICS exceeds the best baseline by 29.9429.9429.9429.94pp in mIoU. Consequently, we highlight the versatility of our method to balance plasticity and rigidity in all tested CISS settings. DKD [baek2022_dkd] achieves the highest benchmark on CISS from the background in our setting but does not generalize on the bifurcation of previously observed classes which we reason with frozen layers in incremental steps. The MicroSeg approach also does not result in a favorable performance on Cityscapes and Mapillary Vistas 2.0 which is caused by low-quality proposals retrieved from the COCO dataset in addition to fully freezing the backbone. Subsequently, we present the mIoU over different increments for the 10-1(10 tasks) scenario on Cityscapes in Fig. 5. We note that for most benchmarks the mIoU increasingly deteriorates after incremental step 3. On the other hand, TOPICS achieves a constant performance from step 1 to step 6 after which the performance again stabilizes. Therefore, our approach supremely maintains base as well as incremental class knowledge while not restricting the adaptability to new classes. We present the results on Mapillary Vistas 2.0 in Tab. II. TOPICS outperforms all baselines by at least 1.61.61.61.6pp on all CISS benchmarks. We justify these results with the rigidity of our model towards base classes while not restraining the learning of new knowledge. Our method significantly outperforms the baselines on novel IoU where we record an improvement of at least 3.53.53.53.5pp. Further, the baselines significantly underperform on the CISS from known classes setting which"}, "abstract": "Semantic segmentation models are typically trained on a fixed set of classes,\nlimiting their applicability in open-world scenarios. Class-incremental\nsemantic segmentation aims to update models with emerging new classes while\npreventing catastrophic forgetting of previously learned ones. However,\nexisting methods impose strict rigidity on old classes, reducing their\neffectiveness in learning new incremental classes. In this work, we propose\nTaxonomy-Oriented Poincar\\'e-regularized Incremental-Class Segmentation\n(TOPICS) that learns feature embeddings in hyperbolic space following explicit\ntaxonomy-tree structures. This supervision provides plasticity for old classes,\nupdating ancestors based on new classes while integrating new classes at\nfitting positions. Additionally, we maintain implicit class relational\nconstraints on the geometric basis of the Poincar\\'e ball. This ensures that\nthe latent space can continuously adapt to new constraints while maintaining a\nrobust structure to combat catastrophic forgetting. We also establish eight\nrealistic incremental learning protocols for autonomous driving scenarios,\nwhere novel classes can originate from known classes or the background.\nExtensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0\nbenchmarks demonstrate that it achieves state-of-the-art performance. We make\nthe code and trained models publicly available at\nhttp://topics.cs.uni-freiburg.de."}, "2407.18178v1": {"title": "PianoMime: Learning a Generalist, Dexterous Piano Player from Internet Demonstrations", "authors": "Cheng Qian, Julen Urain, Kevin Zakka, Jan Peters", "impact_score": 1.609, "keywords": ["Piano", "AI", "Music"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18178v1", "full_text": {"introduction": "", "methodology": "approaches have been limited to low dexterity in the robots or to a small variety of goals. In this work, we focus on the task of learning a generalist piano player from Internet demonstrations. Piano-playing is a highly dexterous open-ended task [11]. Given two multi-fingered robot hands and a desired song, the goal of a piano-playing agent is to press the correct keys and only the correct keys at the proper timing. Moreover, the task can be conditioned on arbitrary songs, allowing for a large, and high-dimensional goal conditioning. Additionally, the Internet is full of videos of professional piano players performing a wide myriad of songs. Interestingly, these piano players often record themselves from a top-view allowing an easy observation of the demonstrations. Additionally, they usually share the MIDI files of the song they play, facilitating the extraction of relevant information. To learn a generalist piano-playing agent from internet data, we introduce PianoMime, a framework to train a single policy capable of playing any song (See Figure 1). In its essence, the PianoMime agent is a goal-conditioned policy that generates configuration space actions given the desired song to be played. At each timestep, the agent receives as goal input a trajectory of the keys to be pressed. Then, the policy generates a trajectory of actions and executes them in chunk. To learn the agent, we combine both reinforcement learning with imitation learning. We train individual song-specific expert policies by using reinforcement learning in conjunction with Youtube demonstrations and we distill all the expert policies into a single generalist behavioral cloning policy. To represent the agent, we perform ablations of different architectural design strategies to model the behavioral cloning policy. We investigate the benefit of incorporating representation learning to enhance the geometric information of the goal input. Additionally, we explore the effectiveness of a hierarchical policy that combines a high-level policy generating fingertip trajectories with a learned cross-domain inverse dynamics model generating joint-space actions. We show that the learned agent is able to play arbitrary songs not included in the training dataset with around 56% F1-score. In summary, the main contribution of this work is a framework for training a generalist piano-playing agent using Internet demonstration data. To achieve this goal, we: Introduce a method to learn policies from the internet demonstrations by decoupling the human motion information from the task-related information. Present a reinforcement learning approach that combines residual policy learning strategies [12, 13] with style reward-based strategies [5]. Explore different policy architecture designs, introducing novel strategies to learn geometrically consistent latent features and conducting ablations on different architectural designs. Finally, we are releasing the dataset and the trained models as a benchmark for testing internet-data-driven dexterous manipulation. Robotic Piano Playing Several studies have investigated the development of robots capable of playing the piano. In [14], multiple-targets Inverse Kinematics (IK) and offline trajectory planning are utilized to position the fingers above the intended keys. In [15], a Reinforcement Learning (RL) agent is trained to control a single Allegro hand to play the piano using tactile sensor feedback. However, the piano pieces used in these studies are relatively simple. Subsequently, in [11], an RL agent is trained to control two Shadow Hands to play complex piano pieces by designing a reward function comprising a fingering reward, a task reward, and an energy reward. In contrast with previous approaches, our approach exploits Youtube piano-playing videos, enabling faster training and more accurate and human-like robot behavior. Motion Retargeting and Reinforcement Learning Our work shares similarities with motion retargeting [16], specifically with those works that combine motion retargeting with RL to learn control policies [17, 18, 5, 19, 6]. Given a mocap demonstration, it has been common to exploit the demonstration rather as a reward function [5, 19] or as a nominal behavior for residual policy learning [18, 6]. In our work, we not only extract the mocap information, but also task-related information (piano states) allowing the agent to balance between mimicking the demonstrations and solving the task. The PianoMime framework is composed of three phases: data preparation, policy learning, and policy distillation. In the data preparation phase, given the raw video demonstration, we extract the informative signals needed to train the policies. Specifically, we extract fingertip trajectories and a MIDI file that informs the piano state at every time instant. In the policy learning phase, we train song-specific policies via RL. This step is essential for generating the robot actions that are missing in the demonstrations. The policy is trained with two reward functions: a style reward and a task reward. The style reward aims to match the robot\u2019s finger movements with those of the human in the demonstrations to preserve the human style, while the task reward encourages the robot to press the correct keys at the proper timing. In the policy distillation phase, we train a single behavioral cloning policy to mimic all the song-specific policies. The goal of this phase is to train a single generalist policy capable of playing any song. We explore different policy designs and the representation learning of goals to improve the generalization capability of the policy. We generate the training dataset by web scraping. We download YouTube videos of professional piano artists playing various songs. We particularly choose YouTube channels that also upload MIDI files of the played songs. The MIDI files represent trajectories of the piano state (pressed/unpressed keys) throughout the song. We use the video to extract the motion of human pianists and the MIDI file to inform about the goal state of piano during the execution of the song. We select the fingertip position as the essential signal to mimic with the robot hand. While several dexterous tasks might require the use of the palm (e.g. grasping a bottle), we consider mimicking the fingertip motion to be sufficient for the task of piano playing. This will also reduce the constraints applied to the robot, allowing it to adapt its embodiment more freely. To extract the fingertip motion from videos, we use MediaPipe", "model_architecture": "", "results": "results in a larger discrepancy between the fingertip trajectory of the robot and the human. Thus, the weight of the style-mimicking reward can be viewed as a parameter that controls the human likeness of the learned robot actions. Qualitative comparison. We present a qualitative comparison of the human likeness of the robot motion in Figure 4 and the attached videos. We inspect the hand poses for certain frames and observe that the IK nominal behavior leads the robot to place the fingers in positions similar to those in Youtube videos. The RL policy then slightly adapts the fingertip positions to press the keys correctly. This section focuses on the evaluation of policy distillation for playing different songs. We evaluate the influence of different policy design strategies on the agent\u2019s performance. We aim to assess (1) the impact of integrating a pre-trained observation encoder to induce spatially consistent features, (2) the impact of a hierarchical design of the policy, and (3) the performance of different generative models on piano-playing data. We propose two base policies, Two-stage Diff and Two-stage Diff-res policy. Both of them utilize hierarchical policies and goal representation learning, as described in Section 3.3. The only difference between them is: the low-level policy of Two-stage Diff directly predicts the target joints, while Two-stage Diff-res predicts the residual term of an IK solver. Both high- and low-level policies are trained with Denoising Diffusion Probabilistic Models (DDPM) [28]. The high-level policy is trained to predict the fingertip trajectory for 4 timesteps given the SDF embedding (See Footnote 3) of goals over 10 timesteps, while the low-level policy predicts the robot actions or residuals for 4 timesteps given the fingertip trajectory. Note that the entire dataset is used for training the high-level policy, while only around 40 % of the collected clips (110K state-action pairs) are trained with RL and further used for training the low-level policy. The detailed network implementation is described in Appendix F. Then, to analyze the impact of each variable, we design four variants of the Two-stage Diffusion policy. To evaluate (1) the impact of integrating a pre-trained observation encoder, we train a model without the SDF embedding representation for the goal (w/o SDF). To evaluate (2) the impact of the hierarchical architecture, we train a One-stage Diffusion policy that directly predict the joint space actions given the goal. Finally, to evaluate (3) the influence of using different generative models, we train a Two-stage BeT, that replaces Diffusion models with Behavior-Transformers [23]. We also consider as baselines a Multi-task RL policy and a BC policy with MSE Loss. We provide further details of the models in Appendix G. Results As shown in Section 4.2, despite that Multi-task RL has the highest precision on the test dataset (this is because it barely presses any keys), our methods (Two-stage Diff and Two-stage Diff-res) outperform the others in all metrics on both training and test datasets. We also observe that the incorporation of SDF embedding for goal representation leads to better performance, especially on the test dataset, which demonstrates the impact of goal representation on policy generalization. Furthermore, we observe a slight performance enhancement when the model predicts the residual term of IK (Two-stage Diff-res). We speculate that this improvement stems from our training data being generated through residual RL, which leverages the output of the IK solver as a prior. This approach likely causes the learned actions to correlate with the outputs of the IK solver. colspec=lc\u2014 cc \u2014 cc \u2014 c \u2014 c \u2014 c, column1-2=font=, column2-70=c,font= \\SetCell[c=2,r=1]l & Multi-RL BC-MSE Two-Stage Diff -res w/o SDF One-Stage BeT \\SetCell[c=1,r=3]l Train P 0.85 0.56 0.87 0.89 0.86 0.53 0.63 R 0.20 0.29 0.78 0.80 0.76 0.34 0.42 F1 0.12 0.30 0.81 0.82 0.78 0.35 0.49 \\SetCell[c=1,r=3]l Test P 0.95 0.54 0.69 0.71 0.66 0.58 0.53 R 0.18 0.22 0.54 0.55 0.49 0.27 0.30 F1 0.13 0.21 0.56 0.57 0.51 0.26 0.31 In this section, we investigate the impact of scaling training data on the generalization capabilities of the agent. We evaluate three policy designs (One-stage Diff, Two-stage Diff, and Two-stage Diff-res). We train them using various proportions of the dataset, and evaluate their performance on the test dataset (see Figure 5 Top). Note that One-stage Diff uses the same dataset as the low-level policy of Two-stage Diff. Results. We observe that both Two-stage Diff and Two-stage Diff-res show consistent performance improvement with increasing used data. This trend implies that the two-stage policies have not yet reached their performance saturation with the given data and could potentially continue to benefit from additional training data in future works. Evaluation on imbalance training datasets. We further employ different combinations of the high-level and low-level policies of Two-stage Diff trained with different proportions of the dataset and assess their performance. In addition, we introduce an oracle high-level policy, which outputs the ground-truth fingertip position from human demonstration videos. The results (see Figure 5 Bottom) demonstrate that the overall performance of policy is significantly influenced by the quality of the high-level policy. Low-level policies paired with Oracle high-level policies consistently outperform the ones paired with other high-level policies. Besides, we observe early performance convergence with increasing training data when paired with a low-quality high-level policy. Specifically, with the HL 1%percent11\\%1 % policy and HL 50%percent5050\\%50 %, performance almost converged with around 10%percent1010\\%10 % and 50%percent5050\\%50 % low-level data, respectively. Inference Speed One of the limitations is the inference speed. The models operate with an inference frequency of approximately 15Hz on an RTX 4090 machine, which is lower than the standard real-time demand on hardware. Future works can employ faster diffusion models, e.g., DDIM [29], to speed up the inference. Out-of-distribution Data Most of the songs in our collected dataset are of modern style. When evaluating the model on the dataset from [11], which mainly contains classical songs, the performance degrades. This discrepancy implies the model\u2019s limited generalization across songs of different styles. Future work can collect more diverse training data to improve this aspect. Acoustic"}, "abstract": "In this work, we introduce PianoMime, a framework for training a\npiano-playing agent using internet demonstrations. The internet is a promising\nsource of large-scale demonstrations for training our robot agents. In\nparticular, for the case of piano-playing, Youtube is full of videos of\nprofessional pianists playing a wide myriad of songs. In our work, we leverage\nthese demonstrations to learn a generalist piano-playing agent capable of\nplaying any arbitrary song. Our framework is divided into three parts: a data\npreparation phase to extract the informative features from the Youtube videos,\na policy learning phase to train song-specific expert policies from the\ndemonstrations and a policy distillation phase to distil the policies into a\nsingle generalist agent. We explore different policy designs to represent the\nagent and evaluate the influence of the amount of training data on the\ngeneralization capability of the agent to novel songs not available in the\ndataset. We show that we are able to learn a policy with up to 56\\% F1 score on\nunseen songs."}, "2407.18175v1": {"title": "Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers", "authors": "Zhengang Li, Alec Lu, Yanyue Xie, Zhenglun Kong, Mengshu Sun, Hao Tang, Zhong Jia Xue, Peiyan Dong, Caiwen Ding, Yanzhi Wang, Xue Lin, Zhenman Fang", "impact_score": 2.359, "keywords": ["Vision Transformers", "Quantization", "Hardware Acceleration"], "citations": 0, "url": "http://arxiv.org/pdf/2407.18175v1", "full_text": {"introduction": "", "methodology": "methods mainly focus on reducing the practical inference latency of matrix multiplication operations. They primarily fall into two categories: 1) neural architecture search (NAS) that searches the lighter-weight model; and 2) model compression, especially model quantization that reduces the per-parameter bit-width. However, there are two major challenges when applying these methods on hardware. The first challenge is associated with model quantization. It has been revealed that the most suitable quantization schemes/bit-widths depend on model sizes and architectures (Wang et al., 2019; Wu et al., 2018), and there is a vast design space in the quantization of both weights and activations for each layer on different models and hardware. As ViT models become deeper, the design space increases exponentially, resulting in poor performance of rule-based strategies. Although recent studies explored automated quantization techniques for a given ViT architecture (Wang et al., 2019; Wu et al., 2018; Uhlich et al., 2020), they did not integrate model quantization with NAS together, which could result in suboptimal performance. In this paper, we propose the framework of model quantization and NAS co-design for ViTs towards improved performance compared to treating NAS and quantization separately. The second challenge is the gap between the theoretical computation throughput and the practical inference speed on actual hardware. For example, layer-wise (inter-layer) mixed-precision quantization (for CNNs) (Wang et al., 2019; Wu et al., 2018) quantizes each layer with a different bit-width and therefore executes layers through distinct hardware components sequentially, leading to low resource utilization. Furthermore, kernel-wise mixed-precision quantization (for CNNs) (Lou et al., 2019) assigns different bit-widths down to the kernel level, significantly diversifying the computing pattern and is inefficient for hardware implementation. Recent work FILM-QNN (Sun et al., 2022) and Auto-ViT-Acc (Li et al., 2022a) leverage the intra-layer mixed quantization to achieve good performance for both model accuracy and throughput on FPGA. By applying two different quantization bit-widths/schemes for different channels and limiting the same mixed-precision ratio across each layer, FPGA can efficiently handle different computations on different hardware resources sharing the same hardware design. However, existing approaches suffer from a manually configured uniform mixed-precision ratio across all layers, potentially compromising quantized model accuracy. Moreover, architectural design considerations are often neglected, limiting the overall model performance. To address these problems comprehensively, we propose Quasar-ViT, an integration of a hardware-oriented quantization-aware architecture search targeting ViT. First, to fully unleash the computation potential of FPGA resources, we investigate a hardware-friendly row-wise mixed-precision quantization scheme. At the algorithm level, different from FILM-QNN (Sun et al., 2022) and Auto-ViT-Acc (Li et al., 2022a), we quantize different channels within each layer into lower and higher bit-widths with the flexibility of different mix-ratios for layers, which achieves a more fine-grained architecture to maintain the accuracy. At the hardware level, we propose the FPGA-based model-adaptive design, including 4-bit atomic computation and hybrid signed/unsigned DSP packing, which set basic hardware units for the lower-bit computation, and decompose the higher-bit computation to lower-bit ones to reuse the resources. Second, during the supernet training, we propose the mixed-precision weight entanglement mechanism, such that different transformer blocks in subnets can share weights for their common parts in each layer to enable efficient quantization during architecture search and reduce training memory cost. On top of that, we establish the corresponding FPGA latency and resource modeling to estimate the inference latency and combine it with an efficient hardware-oriented evolution search method. Based on the above, we integrate with the one-shot NAS algorithm to efficiently find the most accurate quantized model under the given inference latency. We also explore the layer scaling in CaiT (Touvron et al., 2021b) and extend it to the supernet architecture to improve the training efficiency and model accuracy. To demonstrate the compatibility of our proposed framework with knowledge distillation (KD) and further improve our searched model accuracy, we integrate KD (Hinton et al., 2015) into the training process. Finally, on the hardware side, we implement the basic computing units for 4-bit weight and 6-bit activations with hybrid signed/unsigned DSP packing optimization to enable efficient FPGA implementation. The contributions of our work are summarised as follows: An end-to-end hardware-oriented quantization-aware architecture search framework (Quasar-ViT) for ViTs, achieving superior accuracy and inference speed over prior studies. Latency/resource modeling of the hardware accelerator design is integrated into the search process. Hardware-friendly quantization techniques\u2014such as flexible row-wise mixed-precision quantization and mixed-precision weight entanglement\u2014in the architecture search, towards high accuracy, low training cost, and efficient implementation. Real FPGA implementations of our model-adaptive design, with our proposed 4-bit weight atomic computation and hybrid signed/unsigned DSP packing. Integration of proposed supernet layer scaling (SLS) in our framework, achieving further accuracy improvement. Our ablation study also demonstrates our framework\u2019s good compatibility with knowledge distillation (KD). Quasar-ViT achieves 101.5, 159.6 and 251.6 FPS on the AMD/Xilinx ZCU102 FPGA board with 80.4%, 78.6%, and 74.9% top-1 accuracy for ImageNet, respectively. Compared to the representative ViT training-aware quantization (Li et al., 2022b) and the post-training quantization (Liu et al., 2021b), at a similar model size, our model achieves 2.1% and 5.2% higher top-1 accuracy, respectively. Compared with Auto-ViT-Acc (Li et al., 2022a), a state-of-the-art FPGA accelerator for ViT with mixed-scheme quantization (without NAS), we achieve 1.7% better top-1 accuracy with a similar FPS, and 1.6\u00d7\\times\u00d7 better FPS with a similar level of model accuracy. First proposed in (Dosovitskiy et al., 2021), the vision transformer (ViT) is a groundbreaking work that uses transformer blocks for vision tasks. Unlike traditional CNN architectures that use a fixed-size window with restricted spatial interactions, ViT interprets an image as a sequence of patches and adopts the self-attention mechanism (Vaswani et al., 2017). This allows all the positions in an image to interact through transformer blocks, which provides the extraordinary capability to capture relations at the pixel level in both spatial and temporal domains. However, the original ViT requires pre-training with large-scale datasets such as ImageNet-21k and JFT-300M. To tackle the problem, many variants such as DeiT (Touvron et al., 2021a) and T2T-ViT (Yuan et al., 2021a) were proposed, which can be well trained", "model_architecture": "model architecture is transformer encoder blocks with multi-headed self-attention (MSA) and multi-layer perceptron (MLP) blocks. These blocks involve large matrix multiplications, which incur the most computational cost. These complex architectures and enormous computation/storage demand make it hard to deploy ViTs on resource-limited edge devices. Therefore, we quantize all layers involved in matrix multiplication, but not the non-linear functions, e.g., layer normalization, due to their low computational cost and potential effects on accuracy. To compress model size and improve inference speed, model quantization has been widely explored for deep neural networks (DNNs). Existing quantization research can be categorized according to quantization schemes, such as binary (Courbariaux et al., 2015; Rastegari et al., 2016), ternary (He and Fan, 2019), and low-bit-width fixed-point (Zhou et al., 2016; Choi et al., 2018; Zhou et al., 2016; Choi et al., 2018) quantize models with the same interval between each quantization level. Although binary and ternary quantization reduce operations and simplify hardware implementation to the extreme, they introduce large accuracy loss due to insufficient bit-width. For example, based on reports from the above works, accuracy typically degrades by >5%absentpercent5>5\\%> 5 % under binary quantization and 2\u22123%2percent32-3\\%2 - 3 % for ternary quantization. To overcome the large accuracy loss coming from insufficient bit-width, the fixed-point quantization is proposed, applying moderate and adjustable quantization bit-width, to maintain accuracy. This quantization scheme was implemented with different methods and algorithms, such as DoReFa-Net (Zhou et al., 2016) and PACT (Choi et al., 2018). Finally, there are also non-linear quantization schemes, such as power-of-two (PoT) (Leng et al., 2018) and additive PoT (Li et al., 2020). They replace the multiplication with shifting operations where the distribution of quantization levels becomes unbalanced, having higher precision around the mean and less precision at the two sides. To explore more quantization potential while preserving the model accuracy, Besides the single scheme quantization, some works (Wu et al., 2018; Dong et al., 2019; Uhlich et al., 2020; Wang et al., 2019; Shen et al., 2020) explore inter-layer mixed-precision quantization by assigning different precisions to layers. For example, HAQ (Wang et al., 2019) determines the bit-width of each layer by an agent trained with reinforcement learning. DNAS (Wu et al., 2018) used NAS to search layer-wise bit-width. Furthermore, (Lou et al., 2019) explored intra-layer mixed quantization to enable different precisions or schemes within each layer. Based on them, hardware designs (Chang et al., 2021; Sun et al., 2022) leveraged the intra-layer mixed-precision/mixed-scheme to enable uniformity within each layer, guaranteeing inference acceleration. However, they need to set the same mixed ratio for layers, which limits the model\u2019s accuracy. Quantization has also been studied for transformers, especially for natural language processing (NLP) tasks (Zafrir et al., 2019; Zhang et al., 2020; Bai et al., 2021). Q8BERT (Zafrir et al., 2019) finetuned BERT through 8-bit quantization-aware training. TernaryBERT (Zhang et al., 2020) implemented an approximation-based and loss-aware ternary quantization on BERT. BinaryBERT (Bai et al., 2021) proposed a ternary weight splitting strategy to derive binary BERT with performance as the ternary one. Inspired by those, (Liu et al., 2021b) and (Li et al., 2022a) studied quantization on ViT in computer vision tasks. PTQ (Liu et al., 2021b) evaluated the post-training quantization on ViT and achieved comparable accuracy to the full-precision version. Auto-ViT-acc (Li et al., 2022a) proposed an FPGA-aware framework with mixed-scheme quantization for ViT, which we will compare in the evaluation. FQ-ViT (Lin et al., 2022) proposed power-of-two factor and log-int-softmax to proceed with the ViT quantization. Q-ViT (Li et al., 2022b) used the switchable scale to achieve head-wise ViT mixed quantization. However, these works are all based on full-precision pre-trained models and do not include the dimension of network architecture search. There has been a trend to design efficient DNNs with NAS. In general, NAS can be classified into the following categories according to its search strategy. First, reinforcement learning (RL) methods (Zoph and Le, 2017; Zhong et al., 2018; Zoph et al., 2018; Baker et al., 2017; Liu et al., 2018; Cai et al., 2018; Pham et al., 2018) use recurrent neural networks as predictors validating the accuracy of child networks over a proxy dataset. Second, evolution methods (Real et al., 2019; Miikkulainen et al., 2019) develop a pipeline of parent initialization, population updating, generation, and elimination of offspring to find desired networks. Third, one-shot NAS (Bender et al., 2018; You et al., 2020; Guo et al., 2020) trains a large one-shot model containing all operations and shares the weight parameters with all candidate models. Based on the above work, weight-sharing NAS has become popular due to training efficiency (Yu et al., 2020; Wang et al., 2021a; Sahni et al., 2021). One over-parameterized supernet is trained with weights shared across all sub-networks in the search space. This significantly reduces the computational cost during the search. Although most of the above work focuses on the traditional CNN architectures, such as (Yu et al., 2020) and (Sahni et al., 2021), some works have started investigating the search for efficient ViT networks (Wang et al., 2020b; Li et al., 2021a; Wu et al., 2021; Chen et al., 2021). Among them, Autoformer (Chen et al., 2021) entangles the model weights of different ViT blocks in the same layer during supernet training with an efficient weight-sharing strategy to reduce training model storage consumption as well as training time. Some recent works realize the gap between theoretical computation improvement and practical inference speedup. They investigate the algorithm/hardware co-design and incorporate the inference latency into NAS (Tan et al., 2019; Wu et al., 2019; Li et al., 2021b), which is more accurate than intuitive volume estimation by MAC operations. For example, MnasNet (Tan et al., 2019) and NPAS (Li et al., 2021b) utilize the latency on mobile devices as the reward to perform RL search, where gradient-based NAS work FBNet (Wu et al., 2019) adds a latency term to the loss function. However, these works neither target ViTs nor exploit quantization in the hardware-aware ViT search. Figure 2 classifies quantization with different levels", "results": "results for different targeting FPS in Figure 5. Figure 5 (a) illustrates one iteration of the supernet training process, where the pink area indicates the sampled high precision values and the blue area indicates the sampled low precision values in the supernet. The light blue area indicates the frozen values (currently not sampled) in this iteration. After the supernet training and the hardware-oriented evolution search, we could obtain different models targeting different frames per second (FPS) as shown in Figure 5 (b). For the sake of brevity, we only show the quantized value here. The scaling factor along with other related structures is omitted. We show our search space design in Table 2. Our search components include the overall embedding dimension, the number of transformer layers, the quantization mixed-ratio (i.e., the percentage of 8-bit weights mixed in the layer) for each linear layer, and the hidden dimension and expansion ratio (for MLP) in each ViT encoder block. To accelerate the supernet training process and improve the overall model performance, we partition the large-scale search space into two sub-spaces and encode them into two independent supernets for QUASAR-Small and QUASAR-Large, respectively. By splitting and customizing search space for supernets of different sizes, we mitigate the training interference caused by the huge subnets\u2019 difference. This training strategy has been proved in (Zhao et al., 2021). Such partition allows the search algorithm to concentrate on finding models within a specific hardware inference latency, which can be specialized by users according to their available resources and application requirements. It also reduces gradient conflicts between large and small sub-networks trained via weight-sharing due to gaps in model sizes. In each iteration, we randomly select a quantized ViT architecture from the search space. Then we obtain its weights from the supernet and compute the losses of the subnet. Finally, we update the corresponding weights with the remaining weights frozen. The architecture search space P\ud835\udc43Pitalic_P is encoded in a supernet denoted as \ud835\udcae\u2062(P,WP)\ud835\udcae\ud835\udc43subscript\ud835\udc4a\ud835\udc43\\mathcal{S}(P,W_{P})caligraphic_S ( italic_P , italic_W start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ), where WPsubscript\ud835\udc4a\ud835\udc43W_{P}italic_W start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT is the weight of the supernet that is shared across all the candidate architectures. Algorithm 1 illustrates the training procedure of our supernet. In our hardware-oriented evolution search for crossover, two random candidate architectures are first picked from the top candidates. Then we uniformly choose one block from them in each layer to generate a new architecture. For mutation, a candidate mutates its depth with probability Pdsubscript\ud835\udc43\ud835\udc51P_{d}italic_P start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT first. Then it mutates each block with a probability of Pmsubscript\ud835\udc43\ud835\udc5aP_{m}italic_P start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT to produce a new architecture. Newly produced architectures that do not satisfy the constraints will not be added for the next generation. To evaluate the candidates, we perform hardware latency and resource modeling based on the proposed row-wise flexible mixed-precision quantization scheme. The details of the modeling have been discussed in Section 4.4. To demonstrate the compatibility of our proposed framework with knowledge distillation (KD) and further improve the accuracy of our supernet, we also integrate KD (Hinton et al., 2015) in our training process. We use the pre-trained RegNetY-32G (Radosavovic et al., 2020) with 83.6% top-1 accuracy as different teacher models. We also apply the soft distillation method. Soft distillation (Hinton et al., 2015) minimizes the Kullback-Leibler divergence between the softmax of the teacher and the softmax of the student model. The distillation loss is: where Ztsubscript\ud835\udc4d\ud835\udc61Z_{t}italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and Zssubscript\ud835\udc4d\ud835\udc60Z_{s}italic_Z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT are the logits of the teacher and student models, respectively. \u03c8\ud835\udf13\\psiitalic_\u03c8 is the softmax function. \u03c4\ud835\udf0f\\tauitalic_\u03c4 is the temperature for the distillation, \u03b1\ud835\udefc\\alphaitalic_\u03b1 is the coefficient balancing the Kullback\u2013Leibler divergence loss (LK\u2062Lsubscript\ud835\udc3f\ud835\udc3e\ud835\udc3fL_{KL}italic_L start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT), and the cross-entropy (LC\u2062Esubscript\ud835\udc3f\ud835\udc36\ud835\udc38L_{CE}italic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT) on the ground truth labels y\ud835\udc66yitalic_y in the distillation. Figure 6 presents the overall hardware architecture of the Quasar-ViT accelerator on the ARM-FPGA platform. Below is how each module in ViT is mapped to the hardware in Figure 6. The most time-consuming MSA and MLP modules are accelerated by our GEMM engine on the FPGA, which is similar to the recent Auto-ViT-Acc work (Li et al., 2022a). The lightweight SLS modules right after MSA and MLP layers are also accelerated on the FPGA to avoid time-consuming execution on the ARM CPU. The less time-consuming modules including layer normalization and activation functions (i.e., Softmax or GELU) are executed on the ARM CPU, due to their complex structure for FPGA implementation. The hardware engines on the FPGA and software modules on the ARM CPU exchange data via the shared off-chip memory. As previously mentioned, we mainly focus on the most time-consuming GEMM engine design. Due to the limited on-chip memory capacity and computing resource on the FPGA, for each ViT layer (i.e., MSA and MLP), our GEMM engine processes the input, weight, and output data in tiles: a small tile of the input (tokens) and weight of each ViT layer are first loaded from the off-chip DDR memory to the on-chip buffers, then they are processed by the GEMM engine all on-chip. To improve the performance, the double buffering technique is applied again to overlap the off-chip memory accesses and GEMM computation, shown in Figure 6. Next, we present our design of the basic hardware units in the GEMM engine and the corresponding DSP (digital signal processor) packing optimization, as well as the hardware resource and latency modeling for the tiled GEMM design. One major challenge in the FPGA accelerator design is to efficiently support flexible mixed ratios of different bit-width computations across ViT layers. On one hand, putting multiple copies of hardware accelerator designs for each mixed-ratio (i.e., each layer) simultaneously on the FPGA leads to severe hardware resource contention and under-utilization, since layers are executed sequentially. On the other hand, pre-synthesizing multiple copies of hardware accelerator designs for each layer and reconfiguring the FPGA for each layer incurs significant FPGA reconfiguration overhead. Inspired by the approach proposed in QGTC (Wang et al., 2022) to support arbitrary bit-width computation for quantized graph neural networks on GPUs, in our"}, "abstract": "Vision transformers (ViTs) have demonstrated their superior accuracy for\ncomputer vision tasks compared to convolutional neural networks (CNNs).\nHowever, ViT models are often computation-intensive for efficient deployment on\nresource-limited edge devices. This work proposes Quasar-ViT, a\nhardware-oriented quantization-aware architecture search framework for ViTs, to\ndesign efficient ViT models for hardware implementation while preserving the\naccuracy. First, Quasar-ViT trains a supernet using our row-wise flexible\nmixed-precision quantization scheme, mixed-precision weight entanglement, and\nsupernet layer scaling techniques. Then, it applies an efficient\nhardware-oriented search algorithm, integrated with hardware latency and\nresource modeling, to determine a series of optimal subnets from supernet under\ndifferent inference latency targets. Finally, we propose a series of\nmodel-adaptive designs on the FPGA platform to support the architecture search\nand mitigate the gap between the theoretical computation reduction and the\npractical inference speedup. Our searched models achieve 101.5, 159.6, and\n251.6 frames-per-second (FPS) inference speed on the AMD/Xilinx ZCU102 FPGA\nwith 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively, for the ImageNet\ndataset, consistently outperforming prior works."}, "Differentiable Quantum Architecture Search in Asynchronous Quantum Reinforcement Learning": "## Differentiable Quantum Architecture Search in Asynchronous Quantum Reinforcement Learning\n\n**Authors:** Samuel Yen-Chi Chen\n**Impact Score:** 2.359\n**Keywords:** Quantum Architecture Search, Asynchronous RL, QRL\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18202v1\n\n**Summary:**\nHere is a breakdown of the research paper 'Differentiable Quantum Architecture Search in Asynchronous Quantum Reinforcement Learning' in bullet points:\n\n**1. Methodology**\n- The research addresses the challenge of designing effective quantum reinforcement learning (QRL) models by proposing a differentiable quantum architecture search (DiffQAS) approach.\n- The study leverages gradient-based optimization to simultaneously learn circuit parameters and structural weights, enabling the training of both the circuit structure and its parameters. \n- The study also incorporates asynchronous reinforcement learning (RL) methods to enhance training efficiency by facilitating parallel training.\n\n**2. Content Rating:** 51-64\n\n**3. Rating Explanation:**\n-  While the paper presents a novel approach to automating QRL architecture design, the results are primarily based on numerical simulations and lack experimental validation. \n-  The paper focuses on demonstrating comparable performance to manually-crafted architectures, but doesn't provide a clear quantitative measure of the improvement achieved through DiffQAS.\n\n**4. Key Finding:**\n- The paper reports that the DiffQAS-QRL approach achieves performance comparable to manually-crafted circuit architectures across various environments. \n- The study lacks detailed analysis of the statistical significance of the performance comparisons, and it doesn't fully explore the potential advantages of DiffQAS over existing methods.\n\n**5. Potential Improvement:**\n-  The study could benefit from incorporating experimental validation on real quantum hardware to assess the practical effectiveness of the DiffQAS approach. \n-  Further investigation is needed to quantify the improvement achieved by DiffQAS and its potential advantages over other methods. \n", "Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning": "## Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning\n\n**Authors:** Sindhura Kommu, Yizhi Wang, Yue Wang, Xuan Wang\n**Impact Score:** 2.359\n**Keywords:** scRNA-seq, GRN, graph learning\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18181v1\n\n**Summary:**\nHere is a breakdown of the research paper 'Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning' in bullet points:\n\n**1. Methodology**\n- The research addresses the issue of accurately inferring gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data, which is challenging due to data noise, high sparsity, and limited labeled data.\n- The researchers propose a novel approach called scTransNet that combines a pre-trained single-cell language model (scBERT) with structured knowledge from existing GRNs using graph neural networks (GNNs). This joint graph learning approach leverages both gene expression level constraints and structured biological knowledge, aiming to improve GRN inference accuracy.\n\n**2. Content Rating:** 65-79\n\n**3. Rating Explanation**\n- This research demonstrates a good impact by addressing a relevant and complex issue in bioinformatics. \n- The proposed scTransNet approach integrates existing knowledge with deep learning methods, which has potential to advance the field.\n\n**4. Key Finding**\n- The study reports superior performance of scTransNet compared to current state-of-the-art baselines in GRN inference. \n- This suggests that integrating pre-trained transformer models and structured knowledge from GRNs can significantly improve accuracy in reconstructing GRNs from scRNA-seq data.\n\n**5. Potential improvement**\n- While the paper demonstrates the effectiveness of scTransNet, a more thorough evaluation on diverse scRNA-seq datasets with different cell types and experimental conditions would further validate its robustness and generalizability. \n", "PianoMime: Learning a Generalist, Dexterous Piano Player from Internet Demonstrations": "## PianoMime: Learning a Generalist, Dexterous Piano Player from Internet Demonstrations\n\n**Authors:** Cheng Qian, Julen Urain, Kevin Zakka, Jan Peters\n**Impact Score:** 1.609\n**Keywords:** Piano, AI, Music\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18178v1\n\n**Summary:**\nHere is a breakdown of the research paper 'PianoMime: Learning a Generalist, Dexterous Piano Player from Internet Demonstrations' in bullet points:\n\n**1. Methodology**\n- The paper addresses the challenge of training a generalist piano-playing robot using internet demonstrations. \n- The research design involves three phases: data preparation, policy learning, and policy distillation. The authors leverage Youtube piano-playing videos to extract fingertip trajectories and MIDI files, which are then used to train song-specific expert policies via reinforcement learning. Finally, these policies are distilled into a single generalist behavioral cloning policy.\n- The methodology is novel and potentially impactful, however, the study suffers from limited generalization across different styles of music. The reliance on YouTube videos introduces bias, as the data may not be representative of all piano-playing styles or techniques. \n\n**2. Content Rating**\n- 65\n\n**3. Rating Explanation**\n- The research offers a promising approach to robot piano playing, with a strong methodology and initial promising results. However, the limited generalizability across musical styles and the reliance on potentially biased data warrant further investigation.\n\n**4. Key Findings**\n- The study reports an F1 score of 56% on unseen songs, demonstrating a degree of generalization.\n- The authors also explore the impact of various policy designs, finding that incorporating a pre-trained observation encoder and a hierarchical architecture enhances performance.\n\n**5. Potential Improvements**\n- Further research is needed to improve the model's generalization across different musical styles.\n- The authors could explore ways to mitigate potential biases in the training data, such as incorporating diverse musical styles and techniques. \n", "Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers": "## Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers\n\n**Authors:** Zhengang Li, Alec Lu, Yanyue Xie, Zhenglun Kong, Mengshu Sun, Hao Tang, Zhong Jia Xue, Peiyan Dong, Caiwen Ding, Yanzhi Wang, Xue Lin, Zhenman Fang\n**Impact Score:** 2.359\n**Keywords:** Vision Transformers, Quantization, Hardware Acceleration\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18175v1\n\n**Summary:**\nHere is a breakdown of the research paper 'Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers' in bullet points:\n\n**1. Methodology:**\n- The paper aims to address the challenges of deploying Vision Transformers (ViTs) on resource-limited devices by focusing on reducing inference latency through quantization-aware architecture search.\n- The proposed approach involves training a supernet with flexible mixed-precision quantization and weight entanglement, followed by a hardware-oriented evolution search algorithm that incorporates latency and resource modeling to find optimal subnets for different inference targets.\n\n**2. Content Rating:** 65-79\n\n**3. Rating Explanation:**\n- The paper presents a novel framework for hardware-aware ViT optimization, but it lacks a detailed analysis of the hardware design choices, particularly the FPGA implementation.\n\n**4. Key Finding:**\n- The research demonstrates significant inference speed improvements on the AMD/Xilinx ZCU102 FPGA, achieving 101.5, 159.6, and 251.6 frames-per-second (FPS) with 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively.\n- The study shows the potential of combining NAS and quantization for ViT optimization, highlighting the importance of hardware-aware model search.\n\n**5. Potential Improvement:**\n- A comprehensive evaluation of the FPGA implementation with a detailed analysis of the GEMM engine design, hardware resource utilization, and performance metrics would strengthen the study. \n", "ArtA: Automating Design Space Exploration of Spin Qubit Architectures": "## ArtA: Automating Design Space Exploration of Spin Qubit Architectures\n\n**Authors:** Nikiforos Paraskevopoulos, David Hamel, Aritra Sarkar, Carmen G. Almudever, Sebastian Feld\n**Impact Score:** 2.359\n**Keywords:** quantum, architecture, optimization\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18151v1\n\n**Summary:**\nHere is a breakdown of the research paper 'ArtA: Automating Design Space Exploration of Spin Qubit Architectures' in bullet points:\n\n**1. Methodology**\n- The paper addresses the challenge of finding optimal architectural characteristics for spin qubit processors in quantum computing, which is currently hindered by the time and cost of experimental testing.\n- The researchers developed a Design Space Exploration (DSE) framework called SpinQ, enhanced with an optimization tool called ArtA. ArtA leverages 17 optimization method configurations to explore a vast design space of 29,312 spin qubit architectures, significantly reducing exploration time while maintaining accuracy.\n\n**2. Content Rating:** 65-79\n\n**3. Rating Explanation**\n- The paper exhibits a good impact by presenting a novel DSE framework and optimization tool for spin qubit architectures. However, the study could benefit from a more detailed explanation of the optimization methods employed and a more rigorous validation of their effectiveness. \n\n**4. Key Findings**\n- ArtA demonstrates the ability to identify optimal optimization configurations for specific quantum circuits, achieving up to 99.1% faster exploration times compared to brute force.\n- The study reveals crucial design insights, highlighting the importance of maximizing quantum gate parallelization and prioritizing single-qubit gate parallelization over two-qubit gate parallelization.\n\n**5. Potential Improvement**\n- The paper could provide a more detailed analysis of the performance of each optimization method, including their limitations and suitability for different quantum circuit types.\n- Further investigation into the interplay between architectural features and specific quantum algorithms would be valuable for a more comprehensive understanding of the design space. \n", "StraightLine: An End-to-End Resource-Aware Scheduler for Machine Learning Application Requests": "## StraightLine: An End-to-End Resource-Aware Scheduler for Machine Learning Application Requests\n\n**Authors:** Cheng-Wei Ching, Boyuan Guan, Hailu Xu, Liting Hu\n**Impact Score:** 1.759\n**Keywords:** scheduling, resource allocation, machine learning\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18148v1\n\n**Summary:**\nHere is a breakdown of the research paper 'StraightLine: An End-to-End Resource-Aware Scheduler for Machine Learning Application Requests' in bullet points:\n\n**1. Methodology**\n- The research addresses the issue of efficiently scheduling ML application requests across diverse computing platforms, such as local servers, cloud data centers, and serverless platforms. \n- The proposed solution, StraightLine, utilizes Docker for containerization and an empirical dynamic placing algorithm to allocate optimal resources based on request characteristics (frequency, data size). \n- The methodology relies heavily on experimental results without rigorous statistical analysis, potentially limiting the generalizability and reliability of the findings. The paper lacks a clear definition of the metrics used to assess performance and does not compare StraightLine with existing scheduling algorithms.\n\n**2. Content Rating:** 51-64\n\n**3. Rating Explanation**\n- The research demonstrates potential for improving ML application scheduling but lacks depth in methodology and analysis.\n\n**4. Key Finding**\n- The study found that StraightLine can reduce response time and failure rate for ML application requests compared to traditional approaches.\n- The results suggest that serverless computing with 3GB of provisioned memory is effective for handling high request frequencies but lacks in-depth statistical analysis and generalizability.\n\n**5. Potential Improvement**\n- Further research is needed to rigorously evaluate StraightLine\u2019s performance across diverse workloads and compare it with existing scheduling algorithms. \n", "Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for Open-World Perception": "## Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for Open-World Perception\n\n**Authors:** Julia Hindel, Daniele Cattaneo, Abhinav Valada\n**Impact Score:** 2.359\n**Keywords:** Semantic Segmentation, Continual Learning, Hyperbolic Space\n**Citations:** 0\n**URL:** http://arxiv.org/pdf/2407.18145v1\n\n**Summary:**\nHere is a breakdown of the research paper 'Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for Open-World Perception' in bullet points: \n\n* **Methodology:** \n    - The paper addresses the problem of catastrophic forgetting in semantic segmentation models when learning new classes incrementally, as existing methods impose limitations on the plasticity of old classes. \n    -  It proposes a novel approach called TOPICS, which incorporates hyperbolic space and taxonomy-tree structures to learn feature embeddings, providing plasticity for old classes and integrating new classes at suitable positions. This is achieved by using a Poincar\u00e9 ball and imposing implicit class relational constraints, ensuring the latent space adapts to new constraints while preserving a robust structure. \n    - The study utilizes a comprehensive evaluation framework with eight realistic incremental learning protocols for autonomous driving scenarios, comparing TOPICS with five state-of-the-art methods on the Cityscapes and Mapillary Vistas 2.0 benchmarks.\n\n* **Content Rating:** 65\n\n* **Rating Explanation:** \n    - The study exhibits a solid methodology and presents promising findings with improvements in mIoU across different scenarios. \n    - However, it lacks detailed analysis of the hyperparameters used and may benefit from further exploration of the limitations of the approach.\n\n* **Key Finding:** \n    - The TOPICS approach achieved a significant improvement in mIoU on both Cityscapes and Mapillary Vistas 2.0 datasets, outperforming the baseline methods in both base and novel class IoU measures. \n    - The study emphasizes the importance of balancing plasticity and rigidity in class incremental learning to achieve superior results.\n\n* **Potential Improvement:** \n    - Further investigation is required to explore the potential impact of different curvature values in the Poincar\u00e9 ball. \n    - A thorough exploration of the limitations of the approach in handling complex or highly intertwined class relationships could provide valuable insights. \n"}